{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jour 1-2 : Qualité de Code Automatisée**\n",
    "\n",
    "## **Objectifs des deux premiers jours**\n",
    "\n",
    "Maîtriser Ruff comme outil unique de qualité de code, configurer des pre-commit hooks pour automatiser les vérifications, écrire des tests unitaires robustes avec pytest, et établir un workflow de développement qui garantit la qualité à chaque modification de code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Mise en place de l'environnement qualité**\n",
    "\n",
    "### **Pourquoi la qualité de code est votre meilleur investissement**\n",
    "\n",
    "La qualité de code en Data Engineering n'est pas une option mais une nécessité absolue. Vos pipelines traitent des données critiques pour l'entreprise, s'exécutent souvent sans supervision humaine, et doivent être maintenus par différentes personnes au fil du temps. Un bug dans un pipeline de données peut corrompre des analyses, fausser des rapports financiers, ou causer des pertes importantes.\n",
    "\n",
    "**Impact concret de la qualité de code :**\n",
    "\n",
    "Imaginez un pipeline qui traite les données de ventes quotidiennes d'une entreprise. Sans contrôle qualité, une simple erreur de typage pourrait transformer des montants en euros en centimes, faussant tous les rapports de chiffre d'affaires. Avec des outils de qualité automatisés, cette erreur serait détectée avant même que le code soit partagé avec l'équipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuration de Ruff - L'outil unique moderne**\n",
    "\n",
    "Ruff révolutionne la qualité de code Python en remplaçant une dizaine d'outils différents par un seul outil ultra-rapide. Là où vous deviez auparavant configurer Flake8, Black, isort, Pylint, et d'autres outils séparément, Ruff fait tout en une seule passe.\n",
    "\n",
    "**Installation dans votre projet :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans votre projet module1\n",
    "# cd module1\n",
    "# poetry add --group dev ruff\n",
    "\n",
    "# Vérification de l'installation\n",
    "# poetry run ruff --version\n",
    "\n",
    "# Alternative avec pip si vous n'utilisez pas poetry\n",
    "# pip install ruff\n",
    "# ruff --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration progressive de Ruff :**\n",
    "\n",
    "Créez une configuration Ruff adaptée aux débutants mais évolutive. Cette configuration commence simple et peut être enrichie au fur et à mesure de votre apprentissage.\n",
    "\n",
    "Créez un fichier `pyproject.toml` avec la configuration suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Ruff dans pyproject.toml\n",
    "config_ruff = '''\n",
    "[tool.ruff]\n",
    "# Longueur de ligne standard\n",
    "line-length = 88\n",
    "# Version Python cible\n",
    "target-version = \"py311\"\n",
    "# Dossiers à ignorer\n",
    "extend-exclude = [\n",
    "    \".venv\",\n",
    "    \"data/\",\n",
    "    \"*.egg-info\",\n",
    "    \"__pycache__\",\n",
    "]\n",
    "\n",
    "[tool.ruff.lint]\n",
    "# Règles essentielles pour commencer\n",
    "select = [\n",
    "    \"E\",    # Erreurs de style pycodestyle\n",
    "    \"F\",    # Erreurs logiques pyflakes\n",
    "    \"I\",    # Organisation des imports\n",
    "    \"B\",    # Bugs potentiels\n",
    "    \"UP\",   # Modernisation Python\n",
    "]\n",
    "\n",
    "# Règles à ignorer pour débuter en douceur\n",
    "ignore = [\n",
    "    \"E501\",  # Longueur de ligne (géré par le formateur)\n",
    "]\n",
    "\n",
    "# Règles spécifiques par type de fichier\n",
    "[tool.ruff.lint.per-file-ignores]\n",
    "\"tests/*\" = [\n",
    "    \"B006\",  # Arguments mutables OK dans les tests\n",
    "]\n",
    "\n",
    "[tool.ruff.format]\n",
    "# Configuration du formateur automatique\n",
    "quote-style = \"double\"\n",
    "indent-style = \"space\"\n",
    "'''\n",
    "\n",
    "print(\"Configuration Ruff à copier dans pyproject.toml\")\n",
    "print(config_ruff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilisation quotidienne de Ruff :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commandes Ruff essentielles\n",
    "\n",
    "# Vérification et correction automatique\n",
    "# poetry run ruff check src/ --fix\n",
    "\n",
    "# Formatage automatique du code\n",
    "# poetry run ruff format src/\n",
    "\n",
    "# Commande combinée pour un nettoyage complet\n",
    "# poetry run ruff check src/ --fix && poetry run ruff format src/\n",
    "\n",
    "print(\"Commandes à exécuter dans le terminal :\")\n",
    "print(\"1. poetry run ruff check src/ --fix\")\n",
    "print(\"2. poetry run ruff format src/\")\n",
    "print(\"3. poetry run ruff check src/ --fix && poetry run ruff format src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple de transformation automatique :**\n",
    "\n",
    "Voici comment Ruff améliore automatiquement votre code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant Ruff - Code désordonné\n",
    "code_avant = '''\n",
    "import pandas as pd,numpy as np\n",
    "import os,sys\n",
    "\n",
    "def process_data(file_path):\n",
    "    df=pd.read_csv(file_path)\n",
    "    df['new_col']=df['col1']*df['col2']\n",
    "    return df\n",
    "'''\n",
    "\n",
    "# Après Ruff - Code professionnel\n",
    "code_apres = '''\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Traite les données d'un fichier CSV.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"new_col\"] = df[\"col1\"] * df[\"col2\"]\n",
    "    return df\n",
    "'''\n",
    "\n",
    "print(\"AVANT RUFF:\")\n",
    "print(code_avant)\n",
    "print(\"\\nAPRÈS RUFF:\")\n",
    "print(code_apres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Pre-commit Hooks - Automatisation de la qualité**\n",
    "\n",
    "### **Comprendre les pre-commit hooks par l'analogie**\n",
    "\n",
    "Les pre-commit hooks fonctionnent comme un contrôle de sécurité à l'aéroport. Avant que votre code \"embarque\" vers GitHub, il passe automatiquement par une série de vérifications. Si quelque chose ne va pas, le \"vol\" est annulé et vous devez corriger le problème avant de pouvoir repartir.\n",
    "\n",
    "**Pourquoi automatiser plutôt que faire manuellement :**\n",
    "\n",
    "L'automatisation élimine l'erreur humaine et garantit la cohérence. Même le développeur le plus consciencieux peut oublier de lancer Ruff avant un commit urgent. Avec les pre-commit hooks, c'est impossible d'oublier car le processus est automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installation et configuration des pre-commit hooks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de pre-commit\n",
    "# poetry add --group dev pre-commit\n",
    "\n",
    "# Activation des hooks dans votre repository Git\n",
    "# poetry run pre-commit install\n",
    "\n",
    "print(\"Commandes à exécuter :\")\n",
    "print(\"1. poetry add --group dev pre-commit\")\n",
    "print(\"2. poetry run pre-commit install\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration optimisée pour débutants :**\n",
    "\n",
    "Créez un fichier `.pre-commit-config.yaml` qui commence simple mais reste extensible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pre-commit\n",
    "precommit_config = '''\n",
    "# .pre-commit-config.yaml - Configuration débutant\n",
    "repos:\n",
    "  # Vérifications de base des fichiers\n",
    "  - repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.5.0\n",
    "    hooks:\n",
    "      - id: trailing-whitespace        # Supprime espaces en fin de ligne\n",
    "      - id: end-of-file-fixer         # Assure saut de ligne final\n",
    "      - id: check-yaml                # Valide syntaxe YAML\n",
    "      - id: check-json                # Valide syntaxe JSON\n",
    "      - id: check-merge-conflict      # Détecte conflits Git non résolus\n",
    "\n",
    "  # Ruff pour la qualité Python\n",
    "  - repo: https://github.com/astral-sh/ruff-pre-commit\n",
    "    rev: v0.1.8\n",
    "    hooks:\n",
    "      - id: ruff                      # Vérification et correction\n",
    "        args: [--fix]\n",
    "      - id: ruff-format              # Formatage automatique\n",
    "\n",
    "  # Tests rapides avant commit\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: pytest-quick\n",
    "        name: Tests rapides\n",
    "        entry: poetry run pytest\n",
    "        language: system\n",
    "        types: [python]\n",
    "        args: [tests/, -x, --tb=short]\n",
    "        pass_filenames: false\n",
    "'''\n",
    "\n",
    "print(\"Configuration à copier dans .pre-commit-config.yaml :\")\n",
    "print(precommit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test de votre configuration :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manuel des hooks sur tous les fichiers\n",
    "# poetry run pre-commit run --all-files\n",
    "\n",
    "# Simulation d'un commit pour voir les hooks en action\n",
    "# echo \"print('test')\" > test_file.py\n",
    "# git add test_file.py\n",
    "# git commit -m \"Test des pre-commit hooks\"\n",
    "\n",
    "print(\"Commandes de test :\")\n",
    "print(\"1. poetry run pre-commit run --all-files\")\n",
    "print(\"2. echo \\\"print('test')\\\" > test_file.py\")\n",
    "print(\"3. git add test_file.py\")\n",
    "print(\"4. git commit -m 'Test des pre-commit hooks'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Workflow de développement avec pre-commit**\n",
    "\n",
    "**Scénario typique d'une session de développement :**\n",
    "\n",
    "Vous travaillez sur une nouvelle fonctionnalité de votre pipeline de données. Vous modifiez plusieurs fichiers, ajoutez du code, et êtes prêt à sauvegarder votre travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow typique avec pre-commit\n",
    "workflow_exemple = '''\n",
    "# Vous développez normalement\n",
    "echo \"def nouvelle_fonction(): pass\" >> src/utils.py\n",
    "\n",
    "# Vous tentez de committer\n",
    "git add .\n",
    "git commit -m \"Ajout nouvelle fonction\"\n",
    "\n",
    "# Pre-commit s'exécute automatiquement :\n",
    "# ✅ trailing-whitespace: Passed\n",
    "# ✅ end-of-file-fixer: Passed  \n",
    "# ✅ check-yaml: Passed\n",
    "# ✅ ruff: Fixed (corrections automatiques appliquées)\n",
    "# ✅ ruff-format: Passed\n",
    "# ✅ pytest-quick: Passed\n",
    "\n",
    "# Si des corrections ont été appliquées, vous devez recommitter\n",
    "git add .\n",
    "git commit -m \"Ajout nouvelle fonction\"\n",
    "# ✅ Commit réussi !\n",
    "'''\n",
    "\n",
    "print(\"Exemple de workflow avec pre-commit :\")\n",
    "print(workflow_exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gestion des échecs de pre-commit :**\n",
    "\n",
    "Parfois, les pre-commit hooks détectent des problèmes qu'ils ne peuvent pas corriger automatiquement. Voici comment gérer ces situations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de gestion d'échec\n",
    "echec_exemple = '''\n",
    "# Exemple d'échec de test\n",
    "git commit -m \"Nouvelle fonctionnalité\"\n",
    "# ❌ pytest-quick: Failed\n",
    "# tests/test_utils.py::test_nouvelle_fonction FAILED\n",
    "\n",
    "# Vous corrigez le test\n",
    "# Puis recommittez\n",
    "git add .\n",
    "git commit -m \"Nouvelle fonctionnalité avec tests corrigés\"\n",
    "# ✅ Tous les hooks passent\n",
    "'''\n",
    "\n",
    "print(\"Gestion des échecs :\")\n",
    "print(echec_exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Tests unitaires avec pytest**\n",
    "\n",
    "### **Pourquoi les tests sont cruciaux en Data Engineering**\n",
    "\n",
    "En Data Engineering, vos fonctions traitent des données réelles et critiques. Un bug dans une fonction de calcul peut fausser des millions d'euros de transactions. Un problème dans un pipeline peut corrompre des mois d'analyses. Les tests unitaires sont votre filet de sécurité.\n",
    "\n",
    "**Différence entre tester du code classique et du code Data Engineering :**\n",
    "\n",
    "Le code Data Engineering présente des défis spécifiques : données variables en format et en volume, dépendances externes (bases de données, APIs), transformations complexes avec logique métier, et gestion d'erreurs pour des cas imprévisibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuration pytest pour Data Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des outils de test\n",
    "# poetry add --group dev pytest pytest-cov pytest-mock\n",
    "\n",
    "# Structure des tests\n",
    "# mkdir -p tests/{unit,integration,fixtures}\n",
    "# touch tests/__init__.py\n",
    "# touch tests/conftest.py\n",
    "\n",
    "print(\"Commandes d'installation et setup :\")\n",
    "print(\"1. poetry add --group dev pytest pytest-cov pytest-mock\")\n",
    "print(\"2. mkdir -p tests/{unit,integration,fixtures}\")\n",
    "print(\"3. touch tests/__init__.py\")\n",
    "print(\"4. touch tests/conftest.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuration pytest dans pyproject.toml :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pytest\n",
    "pytest_config = '''\n",
    "[tool.pytest.ini_options]\n",
    "# Dossiers de tests\n",
    "testpaths = [\"tests\"]\n",
    "\n",
    "# Patterns de découverte des tests\n",
    "python_files = [\"test_*.py\", \"*_test.py\"]\n",
    "python_classes = [\"Test*\"]\n",
    "python_functions = [\"test_*\"]\n",
    "\n",
    "# Options par défaut\n",
    "addopts = [\n",
    "    \"-v\",                           # Mode verbeux\n",
    "    \"--tb=short\",                   # Traceback court\n",
    "    \"--cov=src\",                    # Couverture du code source\n",
    "    \"--cov-report=term-missing\",    # Rapport de couverture\n",
    "    \"--cov-fail-under=70\",         # Échec si couverture < 70%\n",
    "]\n",
    "\n",
    "# Marqueurs pour organiser les tests\n",
    "markers = [\n",
    "    \"unit: tests unitaires rapides\",\n",
    "    \"integration: tests d'intégration\",\n",
    "    \"slow: tests lents nécessitant des ressources\",\n",
    "]\n",
    "'''\n",
    "\n",
    "print(\"Configuration pytest à ajouter dans pyproject.toml :\")\n",
    "print(pytest_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Création de fixtures pour données de test**\n",
    "\n",
    "Les fixtures pytest permettent de préparer des données de test réutilisables et cohérentes. En Data Engineering, vous avez besoin de données représentatives mais contrôlées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fixtures pour tests Data Engineering\n",
    "import pandas as pd\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "\n",
    "# Simulation de fixtures (normalement dans tests/conftest.py)\n",
    "def sample_sales_data():\n",
    "    \"\"\"Données de ventes pour les tests.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "        'product_id': range(1, 101),\n",
    "        'quantity': [10, 15, 8, 12, 20] * 20,\n",
    "        'price': [99.99, 149.99, 79.99, 199.99, 299.99] * 20,\n",
    "        'customer_id': [f\"CUST_{i:03d}\" for i in range(1, 101)]\n",
    "    })\n",
    "\n",
    "def empty_dataframe():\n",
    "    \"\"\"DataFrame vide pour tester les cas limites.\"\"\"\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def corrupted_data():\n",
    "    \"\"\"Données corrompues pour tester la gestion d'erreurs.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'date': ['2024-01-01', 'invalid_date', '2024-01-03'],\n",
    "        'quantity': [10, -5, None],  # Quantité négative et nulle\n",
    "        'price': [99.99, 'invalid_price', 149.99]\n",
    "    })\n",
    "\n",
    "# Exemple d'utilisation\n",
    "print(\"Exemple de données de test :\")\n",
    "sample_data = sample_sales_data()\n",
    "print(f\"Échantillon de données : {len(sample_data)} lignes\")\n",
    "print(sample_data.head())\n",
    "\n",
    "print(\"\\nDonnées corrompues pour les tests :\")\n",
    "corrupt_data = corrupted_data()\n",
    "print(corrupt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tests unitaires pour fonctions Data Engineering**\n",
    "\n",
    "**Test d'une fonction de nettoyage de données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fonction à tester\n",
    "def clean_sales_data(df):\n",
    "    \"\"\"Nettoie les données de ventes.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Supprime les lignes avec des valeurs nulles\n",
    "    df_clean = df.dropna()\n",
    "    \n",
    "    # Filtre les quantités positives\n",
    "    if 'quantity' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['quantity'] > 0]\n",
    "    \n",
    "    # Filtre les prix positifs\n",
    "    if 'price' in df_clean.columns:\n",
    "        df_clean = df_clean[pd.to_numeric(df_clean['price'], errors='coerce') > 0]\n",
    "    \n",
    "    return df_clean.reset_index(drop=True)\n",
    "\n",
    "def is_valid_quantity(quantity):\n",
    "    \"\"\"Valide qu'une quantité est positive.\"\"\"\n",
    "    if quantity is None:\n",
    "        return False\n",
    "    try:\n",
    "        return float(quantity) > 0\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Tests de la fonction\n",
    "def test_clean_sales_data_valid_input():\n",
    "    \"\"\"Test de nettoyage avec données valides.\"\"\"\n",
    "    sample_data = sample_sales_data()\n",
    "    result = clean_sales_data(sample_data)\n",
    "    \n",
    "    # Vérifications de base\n",
    "    assert isinstance(result, pd.DataFrame)\n",
    "    assert len(result) <= len(sample_data)  # Peut supprimer des lignes\n",
    "    assert not result.isnull().any().any()  # Pas de valeurs nulles\n",
    "    \n",
    "    # Vérifications métier\n",
    "    assert (result['quantity'] > 0).all()  # Quantités positives\n",
    "    assert (result['price'] > 0).all()     # Prix positifs\n",
    "    \n",
    "    print(\"✅ Test données valides réussi\")\n",
    "\n",
    "def test_clean_sales_data_empty_input():\n",
    "    \"\"\"Test avec DataFrame vide.\"\"\"\n",
    "    empty_df = empty_dataframe()\n",
    "    result = clean_sales_data(empty_df)\n",
    "    \n",
    "    assert isinstance(result, pd.DataFrame)\n",
    "    assert len(result) == 0\n",
    "    \n",
    "    print(\"✅ Test DataFrame vide réussi\")\n",
    "\n",
    "def test_clean_sales_data_corrupted_input():\n",
    "    \"\"\"Test avec données corrompues.\"\"\"\n",
    "    corrupt_data = corrupted_data()\n",
    "    result = clean_sales_data(corrupt_data)\n",
    "    \n",
    "    # Doit filtrer les données invalides\n",
    "    assert len(result) < len(corrupt_data)\n",
    "    if len(result) > 0:\n",
    "        assert (result['quantity'] > 0).all()\n",
    "    \n",
    "    print(\"✅ Test données corrompues réussi\")\n",
    "\n",
    "# Tests paramétrés\n",
    "test_cases = [\n",
    "    (10, True),\n",
    "    (0, False),\n",
    "    (-5, False),\n",
    "    (None, False),\n",
    "]\n",
    "\n",
    "def test_validate_quantity():\n",
    "    \"\"\"Test paramétré de validation des quantités.\"\"\"\n",
    "    for quantity, expected in test_cases:\n",
    "        result = is_valid_quantity(quantity)\n",
    "        assert result == expected, f\"Échec pour {quantity}: attendu {expected}, obtenu {result}\"\n",
    "    print(\"✅ Tests paramétrés réussis\")\n",
    "\n",
    "# Exécution des tests\n",
    "print(\"Exécution des tests unitaires :\")\n",
    "test_clean_sales_data_valid_input()\n",
    "test_clean_sales_data_empty_input()\n",
    "test_clean_sales_data_corrupted_input()\n",
    "test_validate_quantity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test d'une classe de traitement de données :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de classe à tester\n",
    "class DataProcessor:\n",
    "    \"\"\"Classe pour traiter les données de ventes.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.data = None\n",
    "        self.processed_count = 0\n",
    "    \n",
    "    def load_data(self, file_path: str):\n",
    "        \"\"\"Charge les données depuis un fichier CSV.\"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Fichier non trouvé : {file_path}\")\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Traite les données chargées.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Aucune donnée chargée\")\n",
    "        \n",
    "        # Calcul du montant total\n",
    "        if 'quantity' in self.data.columns and 'price' in self.data.columns:\n",
    "            self.data['total_amount'] = self.data['quantity'] * self.data['price']\n",
    "        \n",
    "        self.processed_count = len(self.data)\n",
    "        return self.data\n",
    "\n",
    "# Tests de la classe\n",
    "def test_data_processor_init():\n",
    "    \"\"\"Test d'initialisation.\"\"\"\n",
    "    processor = DataProcessor(\"test_processor\")\n",
    "    \n",
    "    assert processor.name == \"test_processor\"\n",
    "    assert processor.data is None\n",
    "    assert processor.processed_count == 0\n",
    "    \n",
    "    print(\"✅ Test initialisation réussi\")\n",
    "\n",
    "def test_load_data_file_not_found():\n",
    "    \"\"\"Test avec fichier inexistant.\"\"\"\n",
    "    processor = DataProcessor(\"test\")\n",
    "    \n",
    "    try:\n",
    "        processor.load_data(\"nonexistent_file.csv\")\n",
    "        assert False, \"Should have raised FileNotFoundError\"\n",
    "    except FileNotFoundError:\n",
    "        print(\"✅ Test fichier inexistant réussi\")\n",
    "\n",
    "def test_process_data_success():\n",
    "    \"\"\"Test de traitement réussi.\"\"\"\n",
    "    processor = DataProcessor(\"test\")\n",
    "    processor.data = sample_sales_data().copy()\n",
    "    \n",
    "    result = processor.process_data()\n",
    "    \n",
    "    assert result is not None\n",
    "    assert processor.processed_count > 0\n",
    "    assert 'total_amount' in result.columns  # Colonne calculée\n",
    "    \n",
    "    print(\"✅ Test traitement réussi\")\n",
    "\n",
    "def test_process_data_no_data_loaded():\n",
    "    \"\"\"Test de traitement sans données chargées.\"\"\"\n",
    "    processor = DataProcessor(\"test\")\n",
    "    \n",
    "    try:\n",
    "        processor.process_data()\n",
    "        assert False, \"Should have raised ValueError\"\n",
    "    except ValueError as e:\n",
    "        assert \"Aucune donnée chargée\" in str(e)\n",
    "        print(\"✅ Test sans données réussi\")\n",
    "\n",
    "# Exécution des tests\n",
    "print(\"Tests de la classe DataProcessor :\")\n",
    "test_data_processor_init()\n",
    "test_load_data_file_not_found()\n",
    "test_process_data_success()\n",
    "test_process_data_no_data_loaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tests d'intégration pour pipelines complets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de classe Pipeline\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class SalesPipeline:\n",
    "    \"\"\"Pipeline complet de traitement des ventes.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str, output_dir: str):\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        self.processor = DataProcessor(\"sales_pipeline\")\n",
    "    \n",
    "    def run(self) -> bool:\n",
    "        \"\"\"Exécute le pipeline complet.\"\"\"\n",
    "        try:\n",
    "            # Chargement des données\n",
    "            self.processor.load_data(self.input_file)\n",
    "            \n",
    "            # Traitement\n",
    "            processed_data = self.processor.process_data()\n",
    "            \n",
    "            # Nettoyage\n",
    "            clean_data = clean_sales_data(processed_data)\n",
    "            \n",
    "            # Sauvegarde\n",
    "            output_file = os.path.join(self.output_dir, \"processed_sales.csv\")\n",
    "            clean_data.to_csv(output_file, index=False)\n",
    "            \n",
    "            # Résumé\n",
    "            summary = {\n",
    "                \"total_records\": len(clean_data),\n",
    "                \"total_amount\": clean_data['total_amount'].sum() if 'total_amount' in clean_data.columns else 0\n",
    "            }\n",
    "            \n",
    "            summary_file = os.path.join(self.output_dir, \"sales_summary.json\")\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump(summary, f)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur dans le pipeline : {e}\")\n",
    "            return False\n",
    "\n",
    "# Test d'intégration\n",
    "def test_pipeline_end_to_end():\n",
    "    \"\"\"Test du pipeline complet de bout en bout.\"\"\"\n",
    "    \n",
    "    # Création de données de test\n",
    "    test_data = sample_sales_data()\n",
    "    \n",
    "    # Création de fichiers temporaires\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Fichier d'entrée\n",
    "        input_file = os.path.join(temp_dir, \"input_sales.csv\")\n",
    "        test_data.to_csv(input_file, index=False)\n",
    "        \n",
    "        # Dossier de sortie\n",
    "        output_dir = os.path.join(temp_dir, \"output\")\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "        # Exécution du pipeline\n",
    "        pipeline = SalesPipeline(input_file, output_dir)\n",
    "        result = pipeline.run()\n",
    "        \n",
    "        # Vérifications\n",
    "        assert result is True\n",
    "        assert os.path.exists(os.path.join(output_dir, \"processed_sales.csv\"))\n",
    "        assert os.path.exists(os.path.join(output_dir, \"sales_summary.json\"))\n",
    "        \n",
    "        # Vérification du contenu\n",
    "        processed_data = pd.read_csv(os.path.join(output_dir, \"processed_sales.csv\"))\n",
    "        assert len(processed_data) > 0\n",
    "        assert 'total_amount' in processed_data.columns\n",
    "        \n",
    "        print(\"✅ Test pipeline end-to-end réussi\")\n",
    "        print(f\"   - {len(processed_data)} lignes traitées\")\n",
    "        print(f\"   - Montant total : {processed_data['total_amount'].sum():.2f}\")\n",
    "\n",
    "# Exécution du test d'intégration\n",
    "print(\"Test d'intégration du pipeline :\")\n",
    "test_pipeline_end_to_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exécution et interprétation des tests**\n",
    "\n",
    "**Commandes essentielles pour les tests :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commandes pytest essentielles\n",
    "commandes_pytest = '''\n",
    "# Exécution de tous les tests\n",
    "poetry run pytest\n",
    "\n",
    "# Tests avec couverture détaillée\n",
    "poetry run pytest --cov=src --cov-report=html\n",
    "\n",
    "# Tests spécifiques\n",
    "poetry run pytest tests/unit/test_data_cleaning.py -v\n",
    "\n",
    "# Tests avec marqueurs\n",
    "poetry run pytest -m \"not slow\"  # Exclut les tests lents\n",
    "\n",
    "# Tests en mode debug\n",
    "poetry run pytest --pdb  # S'arrête au premier échec\n",
    "\n",
    "# Tests parallèles (plus rapide)\n",
    "poetry run pytest -n auto\n",
    "'''\n",
    "\n",
    "print(\"Commandes pytest utiles :\")\n",
    "print(commandes_pytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation des résultats :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de sortie pytest\n",
    "exemple_sortie = '''\n",
    "========================= test session starts =========================\n",
    "collected 15 items\n",
    "\n",
    "tests/unit/test_data_cleaning.py::test_clean_sales_data_valid_input PASSED [ 13%]\n",
    "tests/unit/test_data_cleaning.py::test_clean_sales_data_empty_input PASSED [ 26%]\n",
    "tests/unit/test_data_cleaning.py::test_clean_sales_data_corrupted_input PASSED [ 40%]\n",
    "tests/unit/test_data_processor.py::test_init PASSED [ 53%]\n",
    "tests/unit/test_data_processor.py::test_load_data_success PASSED [ 66%]\n",
    "tests/unit/test_data_processor.py::test_process_data_success PASSED [ 80%]\n",
    "tests/integration/test_pipeline.py::test_pipeline_end_to_end PASSED [ 93%]\n",
    "\n",
    "---------- coverage: platform linux, python 3.11.0 -----------\n",
    "Name                    Stmts   Miss  Cover   Missing\n",
    "-----------------------------------------------------\n",
    "src/data_cleaning.py       45      3    93%   23-25\n",
    "src/data_processor.py      38      2    95%   67, 89\n",
    "src/pipeline.py           52      5    90%   45-49, 78\n",
    "-----------------------------------------------------\n",
    "TOTAL                     135     10    93%\n",
    "\n",
    "========================= 15 passed in 2.34s =========================\n",
    "'''\n",
    "\n",
    "print(\"Exemple de rapport pytest :\")\n",
    "print(exemple_sortie)\n",
    "\n",
    "print(\"\\nInterprétation :\")\n",
    "print(\"- 15 tests collectés et exécutés\")\n",
    "print(\"- Tous les tests sont passés (PASSED)\")\n",
    "print(\"- Couverture de code : 93% (très bon)\")\n",
    "print(\"- Lignes non couvertes identifiées pour amélioration\")\n",
    "print(\"- Temps d'exécution : 2.34 secondes (rapide)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Workflow de développement avec qualité automatisée**\n",
    "\n",
    "### **Intégration dans votre routine quotidienne**\n",
    "\n",
    "Maintenant que vous avez configuré Ruff, pre-commit hooks, et pytest, voici comment ces outils s'intègrent dans votre workflow quotidien de développement.\n",
    "\n",
    "**Routine matinale de développement :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine quotidienne recommandée\n",
    "routine_matinale = '''\n",
    "# 1. Mise à jour du code\n",
    "git pull origin develop\n",
    "\n",
    "# 2. Installation/mise à jour des dépendances\n",
    "poetry install\n",
    "\n",
    "# 3. Vérification que tout fonctionne\n",
    "poetry run pytest tests/ -x\n",
    "\n",
    "# 4. Début du développement\n",
    "poetry run python src/main.py\n",
    "'''\n",
    "\n",
    "print(\"Routine matinale de développement :\")\n",
    "print(routine_matinale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cycle de développement d'une fonctionnalité :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle complet de développement\n",
    "cycle_developpement = '''\n",
    "# 1. Création de la branche de fonctionnalité\n",
    "git checkout -b feature/nouvelle-analyse\n",
    "\n",
    "# 2. Développement avec tests en continu\n",
    "poetry run python src/nouvelle_analyse.py\n",
    "poetry run pytest tests/test_nouvelle_analyse.py -v\n",
    "\n",
    "# 3. Vérification qualité avant commit\n",
    "poetry run ruff check src/ --fix\n",
    "poetry run ruff format src/\n",
    "\n",
    "# 4. Commit avec pre-commit automatique\n",
    "git add .\n",
    "git commit -m \"Ajout analyse des tendances de ventes\"\n",
    "# Pre-commit s'exécute automatiquement\n",
    "\n",
    "# 5. Push vers GitHub\n",
    "git push origin feature/nouvelle-analyse\n",
    "'''\n",
    "\n",
    "print(\"Cycle de développement d'une fonctionnalité :\")\n",
    "print(cycle_developpement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gestion des erreurs et debugging**\n",
    "\n",
    "**Debugging des échecs de tests :**\n",
    "\n",
    "Quand un test échoue, pytest fournit des informations détaillées pour vous aider à comprendre le problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de debugging d'échec de test\n",
    "exemple_debug = '''\n",
    "# Exécution avec informations détaillées\n",
    "poetry run pytest tests/test_data_processor.py::test_process_data_success -vvv\n",
    "\n",
    "# Exemple de sortie d'échec\n",
    "FAILED tests/test_data_processor.py::test_process_data_success - AssertionError\n",
    ">       assert 'total_amount' in result.columns\n",
    "E       AssertionError: assert 'total_amount' in Index(['date', 'quantity', 'price'], dtype='object')\n",
    "\n",
    "# Le test montre que la colonne 'total_amount' n'a pas été créée\n",
    "'''\n",
    "\n",
    "print(\"Exemple de debugging :\")\n",
    "print(exemple_debug)\n",
    "\n",
    "print(\"\\nCorrection guidée par le test :\")\n",
    "correction_code = '''\n",
    "# src/data_processor.py - Correction basée sur l'échec du test\n",
    "def process_data(self):\n",
    "    \"\"\"Traite les données chargées.\"\"\"\n",
    "    if self.data is None:\n",
    "        raise ValueError(\"Aucune donnée chargée\")\n",
    "    \n",
    "    # Ajout de la colonne manquante identifiée par le test\n",
    "    self.data['total_amount'] = self.data['quantity'] * self.data['price']\n",
    "    \n",
    "    self.processed_count = len(self.data)\n",
    "    return self.data\n",
    "'''\n",
    "print(correction_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Résumé et bonnes pratiques**\n",
    "\n",
    "### **Points clés à retenir :**\n",
    "\n",
    "1. **Ruff** : Un seul outil pour toute la qualité de code Python\n",
    "2. **Pre-commit hooks** : Automatisation qui empêche les erreurs d'arriver en production\n",
    "3. **Pytest** : Tests robustes avec fixtures adaptées au Data Engineering\n",
    "4. **Workflow intégré** : Qualité automatique à chaque étape du développement\n",
    "\n",
    "### **Checklist quotidienne :**\n",
    "\n",
    "- [ ] Tests en vert avant de commencer\n",
    "- [ ] Développement avec tests en continu\n",
    "- [ ] Pre-commit hooks activés\n",
    "- [ ] Couverture de code > 70%\n",
    "- [ ] Code formaté automatiquement\n",
    "\n",
    "### **Prochaines étapes :**\n",
    "\n",
    "Vous êtes maintenant équipé pour développer du code Data Engineering de qualité professionnelle. Les jours suivants, nous appliquerons ces bonnes pratiques à des projets concrets de manipulation de données avec Pandas et Polars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}