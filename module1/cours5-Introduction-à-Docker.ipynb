{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jour 5 : Introduction Docker**\n",
    "\n",
    "## **Objectifs du jour 5**\n",
    "\n",
    "Comprendre les concepts fondamentaux de Docker, créer votre premier Dockerfile pour une application Python, containeriser votre pipeline de données, et intégrer Docker dans votre workflow de développement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Concepts fondamentaux de Docker**\n",
    "\n",
    "### **Comprendre Docker par l'analogie du déménagement**\n",
    "\n",
    "Docker résout le problème du \"ça marche sur ma machine\" de la même façon qu'un déménageur professionnel résout le problème du transport de meubles. Imaginez que vous déménagiez et que vous vouliez être sûr que votre bureau fonctionne exactement pareil dans votre nouvelle maison.\n",
    "\n",
    "**Sans Docker (déménagement traditionnel) :** Vous démontez votre bureau, transportez les pièces séparément, et tentez de le remonter dans la nouvelle maison. Problème : la nouvelle maison a des prises électriques différentes, des dimensions de pièces différentes, et vous avez perdu quelques vis en route. Votre bureau ne fonctionne plus pareil.\n",
    "\n",
    "**Avec Docker (container de déménagement) :** Vous mettez votre bureau complet dans un container hermétique avec tout ce dont il a besoin : alimentation électrique, éclairage, même l'air conditionné. Le container arrive dans la nouvelle maison et votre bureau fonctionne instantanément, exactement comme avant.\n",
    "\n",
    "**Application au développement :**\n",
    "\n",
    "Votre application Python est comme votre bureau. Elle a besoin d'une version spécifique de Python (l'électricité), de packages particuliers (les meubles), et de configurations précises (l'agencement). Docker emballe tout cela dans un container qui fonctionne identiquement sur votre machine de développement, celle de votre collègue, et sur les serveurs de production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Différence entre Image et Container**\n",
    "\n",
    "**Image Docker = Plan de construction :** Une image Docker est comme un plan d'architecte détaillé. Elle contient toutes les instructions pour construire votre environnement : quelle version de Python installer, quels packages ajouter, comment configurer l'application. L'image est statique, elle ne change pas.\n",
    "\n",
    "**Container Docker = Maison construite :** Un container est une instance en cours d'exécution d'une image. C'est comme une maison construite à partir du plan. Vous pouvez avoir plusieurs maisons (containers) construites à partir du même plan (image), chacune avec ses propres habitants (processus) et ses propres affaires (données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analogie avec les commandes Docker\n",
    "# docker build -t mon-app .          # Créer le plan (image)\n",
    "# docker run mon-app                 # Construire une maison (container)\n",
    "# docker run mon-app                 # Construire une deuxième maison identique\n",
    "\n",
    "print(\"Commandes Docker de base :\")\n",
    "print(\"docker build -t mon-app .    # Créer une image\")\n",
    "print(\"docker run mon-app           # Lancer un container\")\n",
    "print(\"docker ps                    # Voir les containers actifs\")\n",
    "print(\"docker images                # Voir les images disponibles\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Avantages concrets pour Data Engineering**\n",
    "\n",
    "**Reproductibilité garantie :** Votre pipeline de données fonctionne avec pandas 2.0.1, numpy 1.24.3, et Python 3.11.2. Docker garantit que ces versions exactes seront utilisées partout, éliminant les bugs liés aux différences d'environnement.\n",
    "\n",
    "**Isolation complète :** Votre pipeline peut utiliser PostgreSQL 15 pendant qu'un autre projet sur le même serveur utilise PostgreSQL 12. Docker isole complètement les environnements.\n",
    "\n",
    "**Déploiement simplifié :** Plus besoin d'installer Python, configurer les dépendances, et espérer que tout fonctionne sur le serveur de production. Vous déployez un container qui contient déjà tout.\n",
    "\n",
    "**Collaboration facilitée :** Un nouveau développeur peut lancer votre projet en 30 secondes avec `docker run`, sans passer des heures à configurer son environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Création de votre premier Dockerfile**\n",
    "\n",
    "### **Dockerfile pour application Data Engineering**\n",
    "\n",
    "Un Dockerfile est la recette pour construire votre image Docker. Chaque instruction dans le Dockerfile ajoute une couche à votre image, comme empiler des couches de gâteau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Créons un exemple de Dockerfile pour une application Data Engineering\n",
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile pour pipeline de données\n",
    "# Commentaires expliquent chaque étape pour l'apprentissage\n",
    "\n",
    "# Étape 1: Choisir l'image de base\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Pourquoi python:3.11-slim ?\n",
    "# - Version officielle Python maintenue par l'équipe Docker\n",
    "# - \"slim\" = version allégée sans packages inutiles\n",
    "# - Plus petite et plus sécurisée que l'image complète\n",
    "\n",
    "# Étape 2: Métadonnées de l'image\n",
    "LABEL maintainer=\"votre.email@example.com\"\n",
    "LABEL description=\"Pipeline de données pour analyse des ventes\"\n",
    "LABEL version=\"1.0.0\"\n",
    "\n",
    "# Étape 3: Variables d'environnement pour optimiser Python\n",
    "ENV PYTHONUNBUFFERED=1 \\\\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\\\n",
    "    PIP_NO_CACHE_DIR=1 \\\\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Explication des variables :\n",
    "# PYTHONUNBUFFERED=1 : Affichage immédiat des logs (crucial pour Docker)\n",
    "# PYTHONDONTWRITEBYTECODE=1 : Pas de fichiers .pyc (réduit la taille)\n",
    "# PIP_NO_CACHE_DIR=1 : Pas de cache pip (économise l'espace)\n",
    "\n",
    "# Étape 4: Installation des dépendances système\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    curl \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\\\n",
    "    && apt-get clean\n",
    "\n",
    "# Étape 5: Création d'un utilisateur non-root (sécurité)\n",
    "RUN groupadd --gid 1000 datauser && \\\\\n",
    "    useradd --uid 1000 --gid datauser --shell /bin/bash --create-home datauser\n",
    "\n",
    "# Étape 6: Installation de Poetry\n",
    "RUN pip install poetry==1.7.1\n",
    "\n",
    "# Étape 7: Configuration Poetry pour Docker\n",
    "ENV POETRY_NO_INTERACTION=1 \\\\\n",
    "    POETRY_VENV_IN_PROJECT=1 \\\\\n",
    "    POETRY_CACHE_DIR=/tmp/poetry_cache\n",
    "\n",
    "# Étape 8: Définition du répertoire de travail\n",
    "WORKDIR /app\n",
    "\n",
    "# Étape 9: Copie des fichiers de dépendances AVANT le code\n",
    "COPY pyproject.toml poetry.lock ./\n",
    "\n",
    "# Étape 10: Installation des dépendances\n",
    "RUN poetry install --only=main && rm -rf $POETRY_CACHE_DIR\n",
    "\n",
    "# Étape 11: Copie du code source\n",
    "COPY src/ ./src/\n",
    "COPY scripts/ ./scripts/\n",
    "COPY data/ ./data/\n",
    "\n",
    "# Étape 12: Changement des permissions et utilisateur\n",
    "RUN chown -R datauser:datauser /app\n",
    "USER datauser\n",
    "\n",
    "# Étape 13: Port exposé (si votre app a une interface web)\n",
    "EXPOSE 8000\n",
    "\n",
    "# Étape 14: Health check pour monitoring\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import src; print('OK')\" || exit 1\n",
    "\n",
    "# Étape 15: Commande par défaut\n",
    "CMD [\"poetry\", \"run\", \"python\", \"-m\", \"src.main\"]\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le Dockerfile\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile créé avec succès !\")\n",
    "print(\"\\nStructure du Dockerfile :\")\n",
    "print(\"- Image de base : python:3.11-slim\")\n",
    "print(\"- Optimisations Python pour Docker\")\n",
    "print(\"- Utilisateur non-root pour la sécurité\")\n",
    "print(\"- Poetry pour la gestion des dépendances\")\n",
    "print(\"- Health check intégré\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construction et test de votre image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Commandes pour construire et tester l'image Docker\n",
    "print(\"Commandes de construction :\")\n",
    "print(\"docker build -t pipeline-ventes:latest .\")\n",
    "print(\"docker build -t pipeline-ventes:v1.0 --progress=plain .\")\n",
    "print(\"\\nVérification de l'image :\")\n",
    "print(\"docker images | grep pipeline-ventes\")\n",
    "print(\"\\nCommandes de test :\")\n",
    "print(\"docker run --rm pipeline-ventes:latest\")\n",
    "print(\"docker run -it --rm pipeline-ventes:latest /bin/bash\")\n",
    "print(\"docker run --rm -v $(pwd)/data:/app/data pipeline-ventes:latest\")\n",
    "print(\"\\nCommandes de debugging :\")\n",
    "print(\"docker logs mon-pipeline\")\n",
    "print(\"docker exec -it mon-pipeline /bin/bash\")\n",
    "print(\"docker inspect mon-pipeline\")\n",
    "print(\"docker stats mon-pipeline\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Containerisation d'une application Python complète**\n",
    "\n",
    "### **Projet pratique : Pipeline de données containerisé**\n",
    "\n",
    "Créons ensemble un pipeline de données complet qui traite des fichiers CSV, applique des transformations, et génère des rapports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Créons la structure du projet\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Structure du projet\n",
    "project_structure = {\n",
    "    'src': ['__init__.py', 'main.py'],\n",
    "    'src/pipeline': ['__init__.py', 'extractor.py', 'transformer.py', 'loader.py'],\n",
    "    'src/utils': ['__init__.py', 'logger.py'],\n",
    "    'data/input': [],\n",
    "    'data/processed': [],\n",
    "    'data/output': [],\n",
    "    'tests': [],\n",
    "    'scripts': []\n",
    "}\n",
    "\n",
    "# Créer la structure de dossiers\n",
    "for folder, files in project_structure.items():\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    for file in files:\n",
    "        (Path(folder) / file).touch()\n",
    "\n",
    "print(\"Structure du projet créée :\")\n",
    "for folder in project_structure.keys():\n",
    "    print(f\"📁 {folder}/\")\n",
    "    for file in project_structure[folder]:\n",
    "        print(f\"   📄 {file}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Code du pipeline principal - src/main.py\n",
    "main_py_content = '''\n",
    "\"\"\"Point d'entrée du pipeline de données containerisé.\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def create_demo_data(input_dir: Path) -> None:\n",
    "    \"\"\"Crée des données de démonstration.\"\"\"\n",
    "    # Génération de données de ventes fictives\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2024-01-01', periods=1000, freq='D')\n",
    "    \n",
    "    demo_data = pd.DataFrame({\n",
    "        'date': np.random.choice(dates, 1000),\n",
    "        'product_id': np.random.randint(1, 101, 1000),\n",
    "        'product_name': [f\"Produit_{i}\" for i in np.random.randint(1, 101, 1000)],\n",
    "        'quantity': np.random.randint(1, 50, 1000),\n",
    "        'unit_price': np.round(np.random.uniform(10, 500, 1000), 2),\n",
    "        'customer_id': [f\"CUST_{i:04d}\" for i in np.random.randint(1, 501, 1000)],\n",
    "        'region': np.random.choice(['Nord', 'Sud', 'Est', 'Ouest'], 1000)\n",
    "    })\n",
    "    \n",
    "    demo_file = input_dir / \"sales_demo.csv\"\n",
    "    demo_data.to_csv(demo_file, index=False)\n",
    "    print(f\"📝 Fichier de démonstration créé : {demo_file}\")\n",
    "\n",
    "def main() -> int:\n",
    "    \"\"\"Exécute le pipeline de données complet.\"\"\"\n",
    "    logger = logging.getLogger(\"pipeline\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"🚀 Démarrage du pipeline de données\")\n",
    "        \n",
    "        # Configuration des chemins (compatibles Docker)\n",
    "        base_dir = Path(\".\")\n",
    "        input_dir = base_dir / \"data\" / \"input\"\n",
    "        processed_dir = base_dir / \"data\" / \"processed\"\n",
    "        output_dir = base_dir / \"data\" / \"output\"\n",
    "        \n",
    "        # Création des dossiers si nécessaire\n",
    "        for directory in [input_dir, processed_dir, output_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"📁 Dossier vérifié : {directory}\")\n",
    "        \n",
    "        # Vérification des fichiers d'entrée\n",
    "        input_files = list(input_dir.glob(\"*.csv\"))\n",
    "        if not input_files:\n",
    "            logger.warning(\"⚠️  Aucun fichier CSV trouvé dans data/input/\")\n",
    "            logger.info(\"💡 Création d'un fichier de démonstration\")\n",
    "            create_demo_data(input_dir)\n",
    "            input_files = list(input_dir.glob(\"*.csv\"))\n",
    "        \n",
    "        # Traitement de chaque fichier\n",
    "        for input_file in input_files:\n",
    "            logger.info(f\"📊 Traitement de {input_file.name}\")\n",
    "            \n",
    "            # Lecture des données\n",
    "            raw_data = pd.read_csv(input_file)\n",
    "            logger.info(f\"✅ Extraction : {len(raw_data)} lignes\")\n",
    "            \n",
    "            # Transformation simple\n",
    "            clean_data = raw_data.dropna().drop_duplicates()\n",
    "            clean_data['total_amount'] = clean_data['quantity'] * clean_data['unit_price']\n",
    "            clean_data['processed_at'] = pd.Timestamp.now()\n",
    "            logger.info(f\"✅ Transformation : {len(clean_data)} lignes\")\n",
    "            \n",
    "            # Sauvegarde\n",
    "            output_file = output_dir / f\"processed_{input_file.stem}.csv\"\n",
    "            clean_data.to_csv(output_file, index=False)\n",
    "            logger.info(f\"✅ Sauvegarde : {output_file.name}\")\n",
    "        \n",
    "        logger.info(\"🎉 Pipeline terminé avec succès\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Erreur dans le pipeline : {e}\")\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "\n",
    "# Sauvegarder le fichier main.py\n",
    "with open('src/main.py', 'w') as f:\n",
    "    f.write(main_py_content)\n",
    "\n",
    "print(\"✅ Fichier src/main.py créé\")\n",
    "print(\"\\nFonctionnalités du pipeline :\")\n",
    "print(\"- Création automatique de données de démonstration\")\n",
    "print(\"- Lecture et traitement des fichiers CSV\")\n",
    "print(\"- Nettoyage et enrichissement des données\")\n",
    "print(\"- Sauvegarde des résultats\")\n",
    "print(\"- Logging détaillé\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Testons le pipeline localement\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Installer les dépendances nécessaires\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"✅ Dépendances déjà installées\")\n",
    "except ImportError:\n",
    "    print(\"Installation des dépendances...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\"])\n",
    "\n",
    "# Exécuter le pipeline\n",
    "print(\"\\n🚀 Exécution du pipeline...\")\n",
    "exec(open('src/main.py').read())\n",
    "\n",
    "# Vérifier les résultats\n",
    "output_files = list(Path('data/output').glob('*.csv'))\n",
    "print(f\"\\n📊 Fichiers générés : {len(output_files)}\")\n",
    "for file in output_files:\n",
    "    print(f\"   📄 {file.name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Docker Compose pour environnement complet**\n",
    "\n",
    "Pour un projet plus complexe, utilisez Docker Compose pour orchestrer plusieurs services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Créons un fichier docker-compose.yml\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Application principale\n",
    "  pipeline:\n",
    "    build: .\n",
    "    container_name: data-pipeline\n",
    "    volumes:\n",
    "      # Montage des données pour persistance\n",
    "      - ./data:/app/data\n",
    "      - ./logs:/app/logs\n",
    "    environment:\n",
    "      - LOG_LEVEL=INFO\n",
    "      - ENVIRONMENT=development\n",
    "    depends_on:\n",
    "      - database\n",
    "      - redis\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Base de données PostgreSQL\n",
    "  database:\n",
    "    image: postgres:15-alpine\n",
    "    container_name: postgres-db\n",
    "    environment:\n",
    "      POSTGRES_DB: datawarehouse\n",
    "      POSTGRES_USER: datauser\n",
    "      POSTGRES_PASSWORD: datapass123\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Redis pour cache et queues\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: redis-cache\n",
    "    command: redis-server --appendonly yes\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Interface d'administration\n",
    "  adminer:\n",
    "    image: adminer:latest\n",
    "    container_name: db-admin\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    depends_on:\n",
    "      - database\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "# Volumes persistants\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "\n",
    "# Réseau pour communication inter-services\n",
    "networks:\n",
    "  data-network:\n",
    "    driver: bridge\n",
    "'''\n",
    "\n",
    "# Sauvegarder le fichier docker-compose.yml\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(\"✅ Fichier docker-compose.yml créé\")\n",
    "print(\"\\nServices inclus :\")\n",
    "print(\"🐳 pipeline - Application principale\")\n",
    "print(\"🐘 database - PostgreSQL\")\n",
    "print(\"🔴 redis - Cache et queues\")\n",
    "print(\"🔧 adminer - Interface d'administration DB\")\n",
    "print(\"\\nCommandes utiles :\")\n",
    "print(\"docker-compose up -d        # Lancer tous les services\")\n",
    "print(\"docker-compose logs -f      # Voir les logs\")\n",
    "print(\"docker-compose down         # Arrêter tous les services\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Intégration Docker dans le workflow de développement**\n",
    "\n",
    "### **Workflow de développement avec Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Créons des scripts pour faciliter le développement avec Docker\n",
    "import os\n",
    "\n",
    "# Script de développement\n",
    "dev_script = '''\n",
    "#!/bin/bash\n",
    "# scripts/dev-docker.sh\n",
    "# Script pour développement avec Docker\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"🐳 Démarrage de l'environnement de développement Docker\"\n",
    "\n",
    "# Construction de l'image de développement\n",
    "docker build -f Dockerfile.dev -t pipeline-dev .\n",
    "\n",
    "# Lancement avec montage des sources\n",
    "docker run -it --rm \\\\\n",
    "  --name pipeline-dev \\\\\n",
    "  -v $(pwd)/src:/app/src \\\\\n",
    "  -v $(pwd)/tests:/app/tests \\\\\n",
    "  -v $(pwd)/data:/app/data \\\\\n",
    "  -p 8000:8000 \\\\\n",
    "  pipeline-dev\n",
    "\n",
    "echo \"✅ Environnement de développement prêt\"\n",
    "'''\n",
    "\n",
    "# Script de test\n",
    "test_script = '''\n",
    "#!/bin/bash\n",
    "# scripts/test-docker.sh\n",
    "# Tests dans un environnement Docker propre\n",
    "\n",
    "echo \"🧪 Exécution des tests dans Docker\"\n",
    "\n",
    "# Construction de l'image de test\n",
    "docker build -t pipeline-test .\n",
    "\n",
    "# Exécution des tests\n",
    "docker run --rm \\\\\n",
    "  -v $(pwd)/tests:/app/tests \\\\\n",
    "  -v $(pwd)/data:/app/data \\\\\n",
    "  pipeline-test \\\\\n",
    "  python -m pytest tests/ -v\n",
    "\n",
    "echo \"✅ Tests terminés\"\n",
    "'''\n",
    "\n",
    "# Créer le dossier scripts s'il n'existe pas\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "\n",
    "# Sauvegarder les scripts\n",
    "with open('scripts/dev-docker.sh', 'w') as f:\n",
    "    f.write(dev_script)\n",
    "\n",
    "with open('scripts/test-docker.sh', 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# Rendre les scripts exécutables (sur Unix)\n",
    "try:\n",
    "    os.chmod('scripts/dev-docker.sh', 0o755)\n",
    "    os.chmod('scripts/test-docker.sh', 0o755)\n",
    "except:\n",
    "    pass  # Windows n'a pas chmod\n",
    "\n",
    "print(\"✅ Scripts de développement créés\")\n",
    "print(\"📁 scripts/dev-docker.sh - Environnement de développement\")\n",
    "print(\"📁 scripts/test-docker.sh - Tests automatisés\")\n",
    "print(\"\\nUsage :\")\n",
    "print(\"./scripts/dev-docker.sh   # Développement\")\n",
    "print(\"./scripts/test-docker.sh  # Tests\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation des images Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dockerfile multi-stage pour production\n",
    "dockerfile_prod = '''\n",
    "# Dockerfile.prod - Version optimisée production\n",
    "# Stage 1: Builder\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "ENV POETRY_NO_INTERACTION=1 \\\\\n",
    "    POETRY_VENV_IN_PROJECT=1 \\\\\n",
    "    POETRY_CACHE_DIR=/tmp/poetry_cache\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Installation Poetry\n",
    "RUN pip install poetry\n",
    "\n",
    "# Installation des dépendances\n",
    "COPY pyproject.toml poetry.lock ./\n",
    "RUN poetry install --only=main && rm -rf $POETRY_CACHE_DIR\n",
    "\n",
    "# Stage 2: Production\n",
    "FROM python:3.11-slim as production\n",
    "\n",
    "# Métadonnées\n",
    "LABEL maintainer=\"votre.email@example.com\"\n",
    "LABEL description=\"Pipeline de données - Production\"\n",
    "\n",
    "# Variables d'environnement optimisées\n",
    "ENV PYTHONUNBUFFERED=1 \\\\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\\\n",
    "    PATH=\"/app/.venv/bin:$PATH\"\n",
    "\n",
    "# Installation des dépendances système minimales\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\\\n",
    "    && apt-get clean\n",
    "\n",
    "# Création utilisateur non-root\n",
    "RUN groupadd --gid 1000 appuser && \\\\\n",
    "    useradd --uid 1000 --gid appuser --shell /bin/bash --create-home appuser\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copie de l'environnement virtuel depuis le builder\n",
    "COPY --from=builder /app/.venv /app/.venv\n",
    "\n",
    "# Copie du code source\n",
    "COPY --chown=appuser:appuser src/ ./src/\n",
    "COPY --chown=appuser:appuser scripts/ ./scripts/\n",
    "\n",
    "# Changement vers utilisateur non-root\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import src; print('OK')\" || exit 1\n",
    "\n",
    "# Commande par défaut\n",
    "CMD [\"python\", \"-m\", \"src.main\"]\n",
    "'''\n",
    "\n",
    "# Sauvegarder le Dockerfile de production\n",
    "with open('Dockerfile.prod', 'w') as f:\n",
    "    f.write(dockerfile_prod)\n",
    "\n",
    "print(\"✅ Dockerfile de production créé\")\n",
    "print(\"\\nOptimisations incluses :\")\n",
    "print(\"🏗️  Build multi-stage pour réduire la taille\")\n",
    "print(\"👤 Utilisateur non-root pour la sécurité\")\n",
    "print(\"🩺 Health check intégré\")\n",
    "print(\"⚡ Variables d'environnement optimisées\")\n",
    "print(\"\\nComparaison typique des tailles :\")\n",
    "print(\"📊 Image de développement : ~1.2GB\")\n",
    "print(\"📊 Image de production : ~200MB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercices pratiques**\n",
    "\n",
    "### **Exercice 1 : Créer et tester votre premier container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercice 1 : Créer un simple pipeline de données\n",
    "print(\"🎯 Exercice 1 : Créer et tester votre premier container\")\n",
    "print(\"\\nÉtapes à suivre :\")\n",
    "print(\"1. Créer un fichier requirements.txt avec pandas et numpy\")\n",
    "print(\"2. Créer un script Python simple qui lit un CSV\")\n",
    "print(\"3. Créer un Dockerfile pour containeriser le script\")\n",
    "print(\"4. Construire l'image Docker\")\n",
    "print(\"5. Exécuter le container avec des données de test\")\n",
    "\n",
    "# Créer requirements.txt\n",
    "requirements = \"\"\"\n",
    "pandas==2.0.1\n",
    "numpy==1.24.3\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"\\n✅ requirements.txt créé\")\n",
    "print(\"\\nCommandes à exécuter :\")\n",
    "print(\"docker build -t mon-premier-pipeline .\")\n",
    "print(\"docker run --rm -v $(pwd)/data:/app/data mon-premier-pipeline\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercice 2 : Pipeline avec Docker Compose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercice 2 : Créer un environnement complet avec Docker Compose\n",
    "print(\"🎯 Exercice 2 : Pipeline avec Docker Compose\")\n",
    "print(\"\\nObjectif : Créer un pipeline qui :\")\n",
    "print(\"- Lit des données depuis PostgreSQL\")\n",
    "print(\"- Traite les données avec Python\")\n",
    "print(\"- Stocke les résultats dans Redis\")\n",
    "print(\"- Expose une API simple\")\n",
    "\n",
    "# Créer un docker-compose simple pour l'exercice\n",
    "simple_compose = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  app:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    depends_on:\n",
    "      - db\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "\n",
    "  db:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_DB: mydb\n",
    "      POSTGRES_USER: user\n",
    "      POSTGRES_PASSWORD: pass\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "'''\n",
    "\n",
    "with open('docker-compose.simple.yml', 'w') as f:\n",
    "    f.write(simple_compose)\n",
    "\n",
    "print(\"\\n✅ docker-compose.simple.yml créé\")\n",
    "print(\"\\nCommandes pour l'exercice :\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml up -d\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml logs -f\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml down\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Résumé et bonnes pratiques**\n",
    "\n",
    "### **Points clés à retenir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Résumé des bonnes pratiques Docker\n",
    "print(\"📋 Bonnes pratiques Docker pour Data Engineering\")\n",
    "print(\"\\n🔧 Construction d'images :\")\n",
    "print(\"• Utiliser des images de base officielles et légères (python:3.11-slim)\")\n",
    "print(\"• Copier les dépendances avant le code pour optimiser le cache\")\n",
    "print(\"• Utiliser un utilisateur non-root pour la sécurité\")\n",
    "print(\"• Nettoyer les caches et fichiers temporaires\")\n",
    "\n",
    "print(\"\\n🏗️ Structure des projets :\")\n",
    "print(\"• Séparer les environnements (dev, test, prod)\")\n",
    "print(\"• Utiliser des builds multi-stage pour la production\")\n",
    "print(\"• Documenter les Dockerfiles avec des commentaires\")\n",
    "print(\"• Versionner les images avec des tags explicites\")\n",
    "\n",
    "print(\"\\n🔄 Développement :\")\n",
    "print(\"• Monter les volumes pour la persistance des données\")\n",
    "print(\"• Utiliser Docker Compose pour les environnements complexes\")\n",
    "print(\"• Implémenter des health checks\")\n",
    "print(\"• Configurer les logs pour le monitoring\")\n",
    "\n",
    "print(\"\\n⚡ Performance :\")\n",
    "print(\"• Minimiser le nombre de couches\")\n",
    "print(\"• Utiliser .dockerignore pour exclure les fichiers inutiles\")\n",
    "print(\"• Optimiser l'ordre des instructions\")\n",
    "print(\"• Utiliser des images multi-architecture si nécessaire\")\n",
    "\n",
    "print(\"\\n🎯 Prochaines étapes :\")\n",
    "print(\"• Intégrer Docker dans votre CI/CD\")\n",
    "print(\"• Explorer Kubernetes pour l'orchestration\")\n",
    "print(\"• Apprendre Docker Swarm pour le clustering\")\n",
    "print(\"• Implémenter la surveillance et le monitoring\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ressources supplémentaires**\n",
    "\n",
    "### **Documentation et outils**\n",
    "\n",
    "- **Documentation officielle Docker** : https://docs.docker.com/\n",
    "- **Docker Hub** : https://hub.docker.com/\n",
    "- **Docker Compose** : https://docs.docker.com/compose/\n",
    "- **Bonnes pratiques** : https://docs.docker.com/develop/dev-best-practices/\n",
    "\n",
    "### **Outils recommandés**\n",
    "\n",
    "- **Portainer** : Interface graphique pour Docker\n",
    "- **Docker Desktop** : Environnement de développement\n",
    "- **Hadolint** : Linter pour Dockerfiles\n",
    "- **Dive** : Analyser les couches d'images Docker\n",
    "\n",
    "### **Prochains modules**\n",
    "\n",
    "- **Jour 6** : Orchestration avec Kubernetes\n",
    "- **Jour 7** : CI/CD avec Docker\n",
    "- **Jour 8** : Monitoring et observabilité\n",
    "- **Jour 9** : Sécurité des containers\n",
    "- **Jour 10** : Projet final intégré"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}