{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jour 5 : Introduction Docker**\n",
    "\n",
    "## **Objectifs du jour 5**\n",
    "\n",
    "Comprendre les concepts fondamentaux de Docker, cr√©er votre premier Dockerfile pour une application Python, containeriser votre pipeline de donn√©es, et int√©grer Docker dans votre workflow de d√©veloppement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Concepts fondamentaux de Docker**\n",
    "\n",
    "### **Comprendre Docker par l'analogie du d√©m√©nagement**\n",
    "\n",
    "Docker r√©sout le probl√®me du \"√ßa marche sur ma machine\" de la m√™me fa√ßon qu'un d√©m√©nageur professionnel r√©sout le probl√®me du transport de meubles. Imaginez que vous d√©m√©nagiez et que vous vouliez √™tre s√ªr que votre bureau fonctionne exactement pareil dans votre nouvelle maison.\n",
    "\n",
    "**Sans Docker (d√©m√©nagement traditionnel) :** Vous d√©montez votre bureau, transportez les pi√®ces s√©par√©ment, et tentez de le remonter dans la nouvelle maison. Probl√®me : la nouvelle maison a des prises √©lectriques diff√©rentes, des dimensions de pi√®ces diff√©rentes, et vous avez perdu quelques vis en route. Votre bureau ne fonctionne plus pareil.\n",
    "\n",
    "**Avec Docker (container de d√©m√©nagement) :** Vous mettez votre bureau complet dans un container herm√©tique avec tout ce dont il a besoin : alimentation √©lectrique, √©clairage, m√™me l'air conditionn√©. Le container arrive dans la nouvelle maison et votre bureau fonctionne instantan√©ment, exactement comme avant.\n",
    "\n",
    "**Application au d√©veloppement :**\n",
    "\n",
    "Votre application Python est comme votre bureau. Elle a besoin d'une version sp√©cifique de Python (l'√©lectricit√©), de packages particuliers (les meubles), et de configurations pr√©cises (l'agencement). Docker emballe tout cela dans un container qui fonctionne identiquement sur votre machine de d√©veloppement, celle de votre coll√®gue, et sur les serveurs de production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Diff√©rence entre Image et Container**\n",
    "\n",
    "**Image Docker = Plan de construction :** Une image Docker est comme un plan d'architecte d√©taill√©. Elle contient toutes les instructions pour construire votre environnement : quelle version de Python installer, quels packages ajouter, comment configurer l'application. L'image est statique, elle ne change pas.\n",
    "\n",
    "**Container Docker = Maison construite :** Un container est une instance en cours d'ex√©cution d'une image. C'est comme une maison construite √† partir du plan. Vous pouvez avoir plusieurs maisons (containers) construites √† partir du m√™me plan (image), chacune avec ses propres habitants (processus) et ses propres affaires (donn√©es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analogie avec les commandes Docker\n",
    "# docker build -t mon-app .          # Cr√©er le plan (image)\n",
    "# docker run mon-app                 # Construire une maison (container)\n",
    "# docker run mon-app                 # Construire une deuxi√®me maison identique\n",
    "\n",
    "print(\"Commandes Docker de base :\")\n",
    "print(\"docker build -t mon-app .    # Cr√©er une image\")\n",
    "print(\"docker run mon-app           # Lancer un container\")\n",
    "print(\"docker ps                    # Voir les containers actifs\")\n",
    "print(\"docker images                # Voir les images disponibles\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Avantages concrets pour Data Engineering**\n",
    "\n",
    "**Reproductibilit√© garantie :** Votre pipeline de donn√©es fonctionne avec pandas 2.0.1, numpy 1.24.3, et Python 3.11.2. Docker garantit que ces versions exactes seront utilis√©es partout, √©liminant les bugs li√©s aux diff√©rences d'environnement.\n",
    "\n",
    "**Isolation compl√®te :** Votre pipeline peut utiliser PostgreSQL 15 pendant qu'un autre projet sur le m√™me serveur utilise PostgreSQL 12. Docker isole compl√®tement les environnements.\n",
    "\n",
    "**D√©ploiement simplifi√© :** Plus besoin d'installer Python, configurer les d√©pendances, et esp√©rer que tout fonctionne sur le serveur de production. Vous d√©ployez un container qui contient d√©j√† tout.\n",
    "\n",
    "**Collaboration facilit√©e :** Un nouveau d√©veloppeur peut lancer votre projet en 30 secondes avec `docker run`, sans passer des heures √† configurer son environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Cr√©ation de votre premier Dockerfile**\n",
    "\n",
    "### **Dockerfile pour application Data Engineering**\n",
    "\n",
    "Un Dockerfile est la recette pour construire votre image Docker. Chaque instruction dans le Dockerfile ajoute une couche √† votre image, comme empiler des couches de g√¢teau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©ons un exemple de Dockerfile pour une application Data Engineering\n",
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile pour pipeline de donn√©es\n",
    "# Commentaires expliquent chaque √©tape pour l'apprentissage\n",
    "\n",
    "# √âtape 1: Choisir l'image de base\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Pourquoi python:3.11-slim ?\n",
    "# - Version officielle Python maintenue par l'√©quipe Docker\n",
    "# - \"slim\" = version all√©g√©e sans packages inutiles\n",
    "# - Plus petite et plus s√©curis√©e que l'image compl√®te\n",
    "\n",
    "# √âtape 2: M√©tadonn√©es de l'image\n",
    "LABEL maintainer=\"votre.email@example.com\"\n",
    "LABEL description=\"Pipeline de donn√©es pour analyse des ventes\"\n",
    "LABEL version=\"1.0.0\"\n",
    "\n",
    "# √âtape 3: Variables d'environnement pour optimiser Python\n",
    "ENV PYTHONUNBUFFERED=1 \\\\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\\\n",
    "    PIP_NO_CACHE_DIR=1 \\\\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Explication des variables :\n",
    "# PYTHONUNBUFFERED=1 : Affichage imm√©diat des logs (crucial pour Docker)\n",
    "# PYTHONDONTWRITEBYTECODE=1 : Pas de fichiers .pyc (r√©duit la taille)\n",
    "# PIP_NO_CACHE_DIR=1 : Pas de cache pip (√©conomise l'espace)\n",
    "\n",
    "# √âtape 4: Installation des d√©pendances syst√®me\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    curl \\\\\n",
    "    build-essential \\\\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\\\n",
    "    && apt-get clean\n",
    "\n",
    "# √âtape 5: Cr√©ation d'un utilisateur non-root (s√©curit√©)\n",
    "RUN groupadd --gid 1000 datauser && \\\\\n",
    "    useradd --uid 1000 --gid datauser --shell /bin/bash --create-home datauser\n",
    "\n",
    "# √âtape 6: Installation de Poetry\n",
    "RUN pip install poetry==1.7.1\n",
    "\n",
    "# √âtape 7: Configuration Poetry pour Docker\n",
    "ENV POETRY_NO_INTERACTION=1 \\\\\n",
    "    POETRY_VENV_IN_PROJECT=1 \\\\\n",
    "    POETRY_CACHE_DIR=/tmp/poetry_cache\n",
    "\n",
    "# √âtape 8: D√©finition du r√©pertoire de travail\n",
    "WORKDIR /app\n",
    "\n",
    "# √âtape 9: Copie des fichiers de d√©pendances AVANT le code\n",
    "COPY pyproject.toml poetry.lock ./\n",
    "\n",
    "# √âtape 10: Installation des d√©pendances\n",
    "RUN poetry install --only=main && rm -rf $POETRY_CACHE_DIR\n",
    "\n",
    "# √âtape 11: Copie du code source\n",
    "COPY src/ ./src/\n",
    "COPY scripts/ ./scripts/\n",
    "COPY data/ ./data/\n",
    "\n",
    "# √âtape 12: Changement des permissions et utilisateur\n",
    "RUN chown -R datauser:datauser /app\n",
    "USER datauser\n",
    "\n",
    "# √âtape 13: Port expos√© (si votre app a une interface web)\n",
    "EXPOSE 8000\n",
    "\n",
    "# √âtape 14: Health check pour monitoring\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import src; print('OK')\" || exit 1\n",
    "\n",
    "# √âtape 15: Commande par d√©faut\n",
    "CMD [\"poetry\", \"run\", \"python\", \"-m\", \"src.main\"]\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le Dockerfile\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile cr√©√© avec succ√®s !\")\n",
    "print(\"\\nStructure du Dockerfile :\")\n",
    "print(\"- Image de base : python:3.11-slim\")\n",
    "print(\"- Optimisations Python pour Docker\")\n",
    "print(\"- Utilisateur non-root pour la s√©curit√©\")\n",
    "print(\"- Poetry pour la gestion des d√©pendances\")\n",
    "print(\"- Health check int√©gr√©\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Construction et test de votre image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Commandes pour construire et tester l'image Docker\n",
    "print(\"Commandes de construction :\")\n",
    "print(\"docker build -t pipeline-ventes:latest .\")\n",
    "print(\"docker build -t pipeline-ventes:v1.0 --progress=plain .\")\n",
    "print(\"\\nV√©rification de l'image :\")\n",
    "print(\"docker images | grep pipeline-ventes\")\n",
    "print(\"\\nCommandes de test :\")\n",
    "print(\"docker run --rm pipeline-ventes:latest\")\n",
    "print(\"docker run -it --rm pipeline-ventes:latest /bin/bash\")\n",
    "print(\"docker run --rm -v $(pwd)/data:/app/data pipeline-ventes:latest\")\n",
    "print(\"\\nCommandes de debugging :\")\n",
    "print(\"docker logs mon-pipeline\")\n",
    "print(\"docker exec -it mon-pipeline /bin/bash\")\n",
    "print(\"docker inspect mon-pipeline\")\n",
    "print(\"docker stats mon-pipeline\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Containerisation d'une application Python compl√®te**\n",
    "\n",
    "### **Projet pratique : Pipeline de donn√©es containeris√©**\n",
    "\n",
    "Cr√©ons ensemble un pipeline de donn√©es complet qui traite des fichiers CSV, applique des transformations, et g√©n√®re des rapports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©ons la structure du projet\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Structure du projet\n",
    "project_structure = {\n",
    "    'src': ['__init__.py', 'main.py'],\n",
    "    'src/pipeline': ['__init__.py', 'extractor.py', 'transformer.py', 'loader.py'],\n",
    "    'src/utils': ['__init__.py', 'logger.py'],\n",
    "    'data/input': [],\n",
    "    'data/processed': [],\n",
    "    'data/output': [],\n",
    "    'tests': [],\n",
    "    'scripts': []\n",
    "}\n",
    "\n",
    "# Cr√©er la structure de dossiers\n",
    "for folder, files in project_structure.items():\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    for file in files:\n",
    "        (Path(folder) / file).touch()\n",
    "\n",
    "print(\"Structure du projet cr√©√©e :\")\n",
    "for folder in project_structure.keys():\n",
    "    print(f\"üìÅ {folder}/\")\n",
    "    for file in project_structure[folder]:\n",
    "        print(f\"   üìÑ {file}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Code du pipeline principal - src/main.py\n",
    "main_py_content = '''\n",
    "\"\"\"Point d'entr√©e du pipeline de donn√©es containeris√©.\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def create_demo_data(input_dir: Path) -> None:\n",
    "    \"\"\"Cr√©e des donn√©es de d√©monstration.\"\"\"\n",
    "    # G√©n√©ration de donn√©es de ventes fictives\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2024-01-01', periods=1000, freq='D')\n",
    "    \n",
    "    demo_data = pd.DataFrame({\n",
    "        'date': np.random.choice(dates, 1000),\n",
    "        'product_id': np.random.randint(1, 101, 1000),\n",
    "        'product_name': [f\"Produit_{i}\" for i in np.random.randint(1, 101, 1000)],\n",
    "        'quantity': np.random.randint(1, 50, 1000),\n",
    "        'unit_price': np.round(np.random.uniform(10, 500, 1000), 2),\n",
    "        'customer_id': [f\"CUST_{i:04d}\" for i in np.random.randint(1, 501, 1000)],\n",
    "        'region': np.random.choice(['Nord', 'Sud', 'Est', 'Ouest'], 1000)\n",
    "    })\n",
    "    \n",
    "    demo_file = input_dir / \"sales_demo.csv\"\n",
    "    demo_data.to_csv(demo_file, index=False)\n",
    "    print(f\"üìù Fichier de d√©monstration cr√©√© : {demo_file}\")\n",
    "\n",
    "def main() -> int:\n",
    "    \"\"\"Ex√©cute le pipeline de donn√©es complet.\"\"\"\n",
    "    logger = logging.getLogger(\"pipeline\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"üöÄ D√©marrage du pipeline de donn√©es\")\n",
    "        \n",
    "        # Configuration des chemins (compatibles Docker)\n",
    "        base_dir = Path(\".\")\n",
    "        input_dir = base_dir / \"data\" / \"input\"\n",
    "        processed_dir = base_dir / \"data\" / \"processed\"\n",
    "        output_dir = base_dir / \"data\" / \"output\"\n",
    "        \n",
    "        # Cr√©ation des dossiers si n√©cessaire\n",
    "        for directory in [input_dir, processed_dir, output_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"üìÅ Dossier v√©rifi√© : {directory}\")\n",
    "        \n",
    "        # V√©rification des fichiers d'entr√©e\n",
    "        input_files = list(input_dir.glob(\"*.csv\"))\n",
    "        if not input_files:\n",
    "            logger.warning(\"‚ö†Ô∏è  Aucun fichier CSV trouv√© dans data/input/\")\n",
    "            logger.info(\"üí° Cr√©ation d'un fichier de d√©monstration\")\n",
    "            create_demo_data(input_dir)\n",
    "            input_files = list(input_dir.glob(\"*.csv\"))\n",
    "        \n",
    "        # Traitement de chaque fichier\n",
    "        for input_file in input_files:\n",
    "            logger.info(f\"üìä Traitement de {input_file.name}\")\n",
    "            \n",
    "            # Lecture des donn√©es\n",
    "            raw_data = pd.read_csv(input_file)\n",
    "            logger.info(f\"‚úÖ Extraction : {len(raw_data)} lignes\")\n",
    "            \n",
    "            # Transformation simple\n",
    "            clean_data = raw_data.dropna().drop_duplicates()\n",
    "            clean_data['total_amount'] = clean_data['quantity'] * clean_data['unit_price']\n",
    "            clean_data['processed_at'] = pd.Timestamp.now()\n",
    "            logger.info(f\"‚úÖ Transformation : {len(clean_data)} lignes\")\n",
    "            \n",
    "            # Sauvegarde\n",
    "            output_file = output_dir / f\"processed_{input_file.stem}.csv\"\n",
    "            clean_data.to_csv(output_file, index=False)\n",
    "            logger.info(f\"‚úÖ Sauvegarde : {output_file.name}\")\n",
    "        \n",
    "        logger.info(\"üéâ Pipeline termin√© avec succ√®s\")\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur dans le pipeline : {e}\")\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''\n",
    "\n",
    "# Sauvegarder le fichier main.py\n",
    "with open('src/main.py', 'w') as f:\n",
    "    f.write(main_py_content)\n",
    "\n",
    "print(\"‚úÖ Fichier src/main.py cr√©√©\")\n",
    "print(\"\\nFonctionnalit√©s du pipeline :\")\n",
    "print(\"- Cr√©ation automatique de donn√©es de d√©monstration\")\n",
    "print(\"- Lecture et traitement des fichiers CSV\")\n",
    "print(\"- Nettoyage et enrichissement des donn√©es\")\n",
    "print(\"- Sauvegarde des r√©sultats\")\n",
    "print(\"- Logging d√©taill√©\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Testons le pipeline localement\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Installer les d√©pendances n√©cessaires\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"‚úÖ D√©pendances d√©j√† install√©es\")\n",
    "except ImportError:\n",
    "    print(\"Installation des d√©pendances...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\"])\n",
    "\n",
    "# Ex√©cuter le pipeline\n",
    "print(\"\\nüöÄ Ex√©cution du pipeline...\")\n",
    "exec(open('src/main.py').read())\n",
    "\n",
    "# V√©rifier les r√©sultats\n",
    "output_files = list(Path('data/output').glob('*.csv'))\n",
    "print(f\"\\nüìä Fichiers g√©n√©r√©s : {len(output_files)}\")\n",
    "for file in output_files:\n",
    "    print(f\"   üìÑ {file.name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Docker Compose pour environnement complet**\n",
    "\n",
    "Pour un projet plus complexe, utilisez Docker Compose pour orchestrer plusieurs services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©ons un fichier docker-compose.yml\n",
    "docker_compose_content = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Application principale\n",
    "  pipeline:\n",
    "    build: .\n",
    "    container_name: data-pipeline\n",
    "    volumes:\n",
    "      # Montage des donn√©es pour persistance\n",
    "      - ./data:/app/data\n",
    "      - ./logs:/app/logs\n",
    "    environment:\n",
    "      - LOG_LEVEL=INFO\n",
    "      - ENVIRONMENT=development\n",
    "    depends_on:\n",
    "      - database\n",
    "      - redis\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Base de donn√©es PostgreSQL\n",
    "  database:\n",
    "    image: postgres:15-alpine\n",
    "    container_name: postgres-db\n",
    "    environment:\n",
    "      POSTGRES_DB: datawarehouse\n",
    "      POSTGRES_USER: datauser\n",
    "      POSTGRES_PASSWORD: datapass123\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Redis pour cache et queues\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: redis-cache\n",
    "    command: redis-server --appendonly yes\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Interface d'administration\n",
    "  adminer:\n",
    "    image: adminer:latest\n",
    "    container_name: db-admin\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    depends_on:\n",
    "      - database\n",
    "    networks:\n",
    "      - data-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "# Volumes persistants\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "\n",
    "# R√©seau pour communication inter-services\n",
    "networks:\n",
    "  data-network:\n",
    "    driver: bridge\n",
    "'''\n",
    "\n",
    "# Sauvegarder le fichier docker-compose.yml\n",
    "with open('docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "print(\"‚úÖ Fichier docker-compose.yml cr√©√©\")\n",
    "print(\"\\nServices inclus :\")\n",
    "print(\"üê≥ pipeline - Application principale\")\n",
    "print(\"üêò database - PostgreSQL\")\n",
    "print(\"üî¥ redis - Cache et queues\")\n",
    "print(\"üîß adminer - Interface d'administration DB\")\n",
    "print(\"\\nCommandes utiles :\")\n",
    "print(\"docker-compose up -d        # Lancer tous les services\")\n",
    "print(\"docker-compose logs -f      # Voir les logs\")\n",
    "print(\"docker-compose down         # Arr√™ter tous les services\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Int√©gration Docker dans le workflow de d√©veloppement**\n",
    "\n",
    "### **Workflow de d√©veloppement avec Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cr√©ons des scripts pour faciliter le d√©veloppement avec Docker\n",
    "import os\n",
    "\n",
    "# Script de d√©veloppement\n",
    "dev_script = '''\n",
    "#!/bin/bash\n",
    "# scripts/dev-docker.sh\n",
    "# Script pour d√©veloppement avec Docker\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"üê≥ D√©marrage de l'environnement de d√©veloppement Docker\"\n",
    "\n",
    "# Construction de l'image de d√©veloppement\n",
    "docker build -f Dockerfile.dev -t pipeline-dev .\n",
    "\n",
    "# Lancement avec montage des sources\n",
    "docker run -it --rm \\\\\n",
    "  --name pipeline-dev \\\\\n",
    "  -v $(pwd)/src:/app/src \\\\\n",
    "  -v $(pwd)/tests:/app/tests \\\\\n",
    "  -v $(pwd)/data:/app/data \\\\\n",
    "  -p 8000:8000 \\\\\n",
    "  pipeline-dev\n",
    "\n",
    "echo \"‚úÖ Environnement de d√©veloppement pr√™t\"\n",
    "'''\n",
    "\n",
    "# Script de test\n",
    "test_script = '''\n",
    "#!/bin/bash\n",
    "# scripts/test-docker.sh\n",
    "# Tests dans un environnement Docker propre\n",
    "\n",
    "echo \"üß™ Ex√©cution des tests dans Docker\"\n",
    "\n",
    "# Construction de l'image de test\n",
    "docker build -t pipeline-test .\n",
    "\n",
    "# Ex√©cution des tests\n",
    "docker run --rm \\\\\n",
    "  -v $(pwd)/tests:/app/tests \\\\\n",
    "  -v $(pwd)/data:/app/data \\\\\n",
    "  pipeline-test \\\\\n",
    "  python -m pytest tests/ -v\n",
    "\n",
    "echo \"‚úÖ Tests termin√©s\"\n",
    "'''\n",
    "\n",
    "# Cr√©er le dossier scripts s'il n'existe pas\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "\n",
    "# Sauvegarder les scripts\n",
    "with open('scripts/dev-docker.sh', 'w') as f:\n",
    "    f.write(dev_script)\n",
    "\n",
    "with open('scripts/test-docker.sh', 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# Rendre les scripts ex√©cutables (sur Unix)\n",
    "try:\n",
    "    os.chmod('scripts/dev-docker.sh', 0o755)\n",
    "    os.chmod('scripts/test-docker.sh', 0o755)\n",
    "except:\n",
    "    pass  # Windows n'a pas chmod\n",
    "\n",
    "print(\"‚úÖ Scripts de d√©veloppement cr√©√©s\")\n",
    "print(\"üìÅ scripts/dev-docker.sh - Environnement de d√©veloppement\")\n",
    "print(\"üìÅ scripts/test-docker.sh - Tests automatis√©s\")\n",
    "print(\"\\nUsage :\")\n",
    "print(\"./scripts/dev-docker.sh   # D√©veloppement\")\n",
    "print(\"./scripts/test-docker.sh  # Tests\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation des images Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dockerfile multi-stage pour production\n",
    "dockerfile_prod = '''\n",
    "# Dockerfile.prod - Version optimis√©e production\n",
    "# Stage 1: Builder\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "ENV POETRY_NO_INTERACTION=1 \\\\\n",
    "    POETRY_VENV_IN_PROJECT=1 \\\\\n",
    "    POETRY_CACHE_DIR=/tmp/poetry_cache\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Installation Poetry\n",
    "RUN pip install poetry\n",
    "\n",
    "# Installation des d√©pendances\n",
    "COPY pyproject.toml poetry.lock ./\n",
    "RUN poetry install --only=main && rm -rf $POETRY_CACHE_DIR\n",
    "\n",
    "# Stage 2: Production\n",
    "FROM python:3.11-slim as production\n",
    "\n",
    "# M√©tadonn√©es\n",
    "LABEL maintainer=\"votre.email@example.com\"\n",
    "LABEL description=\"Pipeline de donn√©es - Production\"\n",
    "\n",
    "# Variables d'environnement optimis√©es\n",
    "ENV PYTHONUNBUFFERED=1 \\\\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\\\n",
    "    PATH=\"/app/.venv/bin:$PATH\"\n",
    "\n",
    "# Installation des d√©pendances syst√®me minimales\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/* \\\\\n",
    "    && apt-get clean\n",
    "\n",
    "# Cr√©ation utilisateur non-root\n",
    "RUN groupadd --gid 1000 appuser && \\\\\n",
    "    useradd --uid 1000 --gid appuser --shell /bin/bash --create-home appuser\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copie de l'environnement virtuel depuis le builder\n",
    "COPY --from=builder /app/.venv /app/.venv\n",
    "\n",
    "# Copie du code source\n",
    "COPY --chown=appuser:appuser src/ ./src/\n",
    "COPY --chown=appuser:appuser scripts/ ./scripts/\n",
    "\n",
    "# Changement vers utilisateur non-root\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD python -c \"import src; print('OK')\" || exit 1\n",
    "\n",
    "# Commande par d√©faut\n",
    "CMD [\"python\", \"-m\", \"src.main\"]\n",
    "'''\n",
    "\n",
    "# Sauvegarder le Dockerfile de production\n",
    "with open('Dockerfile.prod', 'w') as f:\n",
    "    f.write(dockerfile_prod)\n",
    "\n",
    "print(\"‚úÖ Dockerfile de production cr√©√©\")\n",
    "print(\"\\nOptimisations incluses :\")\n",
    "print(\"üèóÔ∏è  Build multi-stage pour r√©duire la taille\")\n",
    "print(\"üë§ Utilisateur non-root pour la s√©curit√©\")\n",
    "print(\"ü©∫ Health check int√©gr√©\")\n",
    "print(\"‚ö° Variables d'environnement optimis√©es\")\n",
    "print(\"\\nComparaison typique des tailles :\")\n",
    "print(\"üìä Image de d√©veloppement : ~1.2GB\")\n",
    "print(\"üìä Image de production : ~200MB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercices pratiques**\n",
    "\n",
    "### **Exercice 1 : Cr√©er et tester votre premier container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercice 1 : Cr√©er un simple pipeline de donn√©es\n",
    "print(\"üéØ Exercice 1 : Cr√©er et tester votre premier container\")\n",
    "print(\"\\n√âtapes √† suivre :\")\n",
    "print(\"1. Cr√©er un fichier requirements.txt avec pandas et numpy\")\n",
    "print(\"2. Cr√©er un script Python simple qui lit un CSV\")\n",
    "print(\"3. Cr√©er un Dockerfile pour containeriser le script\")\n",
    "print(\"4. Construire l'image Docker\")\n",
    "print(\"5. Ex√©cuter le container avec des donn√©es de test\")\n",
    "\n",
    "# Cr√©er requirements.txt\n",
    "requirements = \"\"\"\n",
    "pandas==2.0.1\n",
    "numpy==1.24.3\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"\\n‚úÖ requirements.txt cr√©√©\")\n",
    "print(\"\\nCommandes √† ex√©cuter :\")\n",
    "print(\"docker build -t mon-premier-pipeline .\")\n",
    "print(\"docker run --rm -v $(pwd)/data:/app/data mon-premier-pipeline\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercice 2 : Pipeline avec Docker Compose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercice 2 : Cr√©er un environnement complet avec Docker Compose\n",
    "print(\"üéØ Exercice 2 : Pipeline avec Docker Compose\")\n",
    "print(\"\\nObjectif : Cr√©er un pipeline qui :\")\n",
    "print(\"- Lit des donn√©es depuis PostgreSQL\")\n",
    "print(\"- Traite les donn√©es avec Python\")\n",
    "print(\"- Stocke les r√©sultats dans Redis\")\n",
    "print(\"- Expose une API simple\")\n",
    "\n",
    "# Cr√©er un docker-compose simple pour l'exercice\n",
    "simple_compose = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  app:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    depends_on:\n",
    "      - db\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n",
    "    volumes:\n",
    "      - ./data:/app/data\n",
    "\n",
    "  db:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_DB: mydb\n",
    "      POSTGRES_USER: user\n",
    "      POSTGRES_PASSWORD: pass\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "'''\n",
    "\n",
    "with open('docker-compose.simple.yml', 'w') as f:\n",
    "    f.write(simple_compose)\n",
    "\n",
    "print(\"\\n‚úÖ docker-compose.simple.yml cr√©√©\")\n",
    "print(\"\\nCommandes pour l'exercice :\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml up -d\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml logs -f\")\n",
    "print(\"docker-compose -f docker-compose.simple.yml down\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **R√©sum√© et bonnes pratiques**\n",
    "\n",
    "### **Points cl√©s √† retenir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# R√©sum√© des bonnes pratiques Docker\n",
    "print(\"üìã Bonnes pratiques Docker pour Data Engineering\")\n",
    "print(\"\\nüîß Construction d'images :\")\n",
    "print(\"‚Ä¢ Utiliser des images de base officielles et l√©g√®res (python:3.11-slim)\")\n",
    "print(\"‚Ä¢ Copier les d√©pendances avant le code pour optimiser le cache\")\n",
    "print(\"‚Ä¢ Utiliser un utilisateur non-root pour la s√©curit√©\")\n",
    "print(\"‚Ä¢ Nettoyer les caches et fichiers temporaires\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Structure des projets :\")\n",
    "print(\"‚Ä¢ S√©parer les environnements (dev, test, prod)\")\n",
    "print(\"‚Ä¢ Utiliser des builds multi-stage pour la production\")\n",
    "print(\"‚Ä¢ Documenter les Dockerfiles avec des commentaires\")\n",
    "print(\"‚Ä¢ Versionner les images avec des tags explicites\")\n",
    "\n",
    "print(\"\\nüîÑ D√©veloppement :\")\n",
    "print(\"‚Ä¢ Monter les volumes pour la persistance des donn√©es\")\n",
    "print(\"‚Ä¢ Utiliser Docker Compose pour les environnements complexes\")\n",
    "print(\"‚Ä¢ Impl√©menter des health checks\")\n",
    "print(\"‚Ä¢ Configurer les logs pour le monitoring\")\n",
    "\n",
    "print(\"\\n‚ö° Performance :\")\n",
    "print(\"‚Ä¢ Minimiser le nombre de couches\")\n",
    "print(\"‚Ä¢ Utiliser .dockerignore pour exclure les fichiers inutiles\")\n",
    "print(\"‚Ä¢ Optimiser l'ordre des instructions\")\n",
    "print(\"‚Ä¢ Utiliser des images multi-architecture si n√©cessaire\")\n",
    "\n",
    "print(\"\\nüéØ Prochaines √©tapes :\")\n",
    "print(\"‚Ä¢ Int√©grer Docker dans votre CI/CD\")\n",
    "print(\"‚Ä¢ Explorer Kubernetes pour l'orchestration\")\n",
    "print(\"‚Ä¢ Apprendre Docker Swarm pour le clustering\")\n",
    "print(\"‚Ä¢ Impl√©menter la surveillance et le monitoring\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ressources suppl√©mentaires**\n",
    "\n",
    "### **Documentation et outils**\n",
    "\n",
    "- **Documentation officielle Docker** : https://docs.docker.com/\n",
    "- **Docker Hub** : https://hub.docker.com/\n",
    "- **Docker Compose** : https://docs.docker.com/compose/\n",
    "- **Bonnes pratiques** : https://docs.docker.com/develop/dev-best-practices/\n",
    "\n",
    "### **Outils recommand√©s**\n",
    "\n",
    "- **Portainer** : Interface graphique pour Docker\n",
    "- **Docker Desktop** : Environnement de d√©veloppement\n",
    "- **Hadolint** : Linter pour Dockerfiles\n",
    "- **Dive** : Analyser les couches d'images Docker\n",
    "\n",
    "### **Prochains modules**\n",
    "\n",
    "- **Jour 6** : Orchestration avec Kubernetes\n",
    "- **Jour 7** : CI/CD avec Docker\n",
    "- **Jour 8** : Monitoring et observabilit√©\n",
    "- **Jour 9** : S√©curit√© des containers\n",
    "- **Jour 10** : Projet final int√©gr√©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}