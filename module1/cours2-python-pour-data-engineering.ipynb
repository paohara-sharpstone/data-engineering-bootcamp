{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 1 : Python pour Data Engineering**\n",
    "\n",
    "## **1. Programmation Orientée Objet - Concepts Fondamentaux**\n",
    "\n",
    "### **Définitions essentielles**\n",
    "\n",
    "La programmation orientée objet repose sur six concepts fondamentaux qu'il est essentiel de maîtriser avant de créer des pipelines de données robustes.\n",
    "\n",
    "| Terme | Définition | Exemple |\n",
    "|-------|------------|------|\n",
    "| **Classe** | Modèle ou plan de construction pour créer des objets | `class Voiture:` |\n",
    "| **Instance** | Un objet créé à partir d'une classe | `ma_voiture = Voiture()` |\n",
    "| **Attribut** | Variable qui appartient à une instance | `ma_voiture.couleur = \"rouge\"` |\n",
    "| **Méthode** | Fonction qui appartient à une classe | `ma_voiture.demarrer()` |\n",
    "| **Constructeur** | Méthode spéciale qui initialise une instance | `__init__(self)` |\n",
    "| **Encapsulation** | Regroupement des données et méthodes dans une classe | Tout dans la classe |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Classe : Le Plan de Construction**\n",
    "\n",
    "Une classe est comme un plan d'architecte. Elle définit la structure et le comportement que partageront tous les objets créés à partir de ce plan. En Data Engineering, nous utilisons les classes pour organiser notre code de manière logique et réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcesseurDonnees:\n",
    "    \"\"\"Plan pour créer des processeurs de données\"\"\"\n",
    "    pass  # Pour l'instant, juste la structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette classe ne fait rien encore, mais elle établit le concept. Tous les processeurs de données que nous créerons à partir de cette classe partageront la même structure de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Instance : L'Objet Concret**\n",
    "\n",
    "Une instance est un objet concret créé à partir d'une classe. C'est comme construire une maison à partir du plan d'architecte. Chaque maison (instance) est unique même si elle suit le même plan (classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de deux instances différentes\n",
    "processeur1 = ProcesseurDonnees()  # Première instance\n",
    "processeur2 = ProcesseurDonnees()  # Deuxième instance\n",
    "\n",
    "# Chaque instance est indépendante\n",
    "print(processeur1 is processeur2)  # False - ce sont des objets différents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Attribut : Les Caractéristiques de l'Objet**\n",
    "\n",
    "Les attributs sont les variables qui appartiennent à une instance. Ils stockent l'état et les données spécifiques à chaque objet. Chaque instance peut avoir des valeurs différentes pour ses attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcesseurDonnees:\n",
    "    def __init__(self, nom):\n",
    "        self.nom = nom           # Attribut d'instance\n",
    "        self.donnees = None      # Attribut d'instance\n",
    "        self.statut = \"inactif\"  # Attribut d'instance\n",
    "\n",
    "# Chaque instance a ses propres attributs\n",
    "processeur_ventes = ProcesseurDonnees(\"Ventes\")\n",
    "processeur_clients = ProcesseurDonnees(\"Clients\")\n",
    "\n",
    "print(processeur_ventes.nom)    # \"Ventes\"\n",
    "print(processeur_clients.nom)   # \"Clients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Méthode : Les Actions de l'Objet**\n",
    "\n",
    "Les méthodes sont les fonctions qui appartiennent à une classe. Elles définissent ce que les objets peuvent faire. Les méthodes peuvent accéder et modifier les attributs de l'instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcesseurDonnees:\n",
    "    def __init__(self, nom):\n",
    "        self.nom = nom\n",
    "        self.donnees = None\n",
    "    \n",
    "    def charger_donnees(self, fichier):  # Méthode\n",
    "        \"\"\"Charge des données depuis un fichier\"\"\"\n",
    "        import pandas as pd\n",
    "        self.donnees = pd.read_csv(fichier)\n",
    "        print(f\"{self.nom}: {len(self.donnees)} lignes chargées\")\n",
    "    \n",
    "    def obtenir_resume(self):  # Méthode\n",
    "        \"\"\"Retourne un résumé des données\"\"\"\n",
    "        if self.donnees is not None:\n",
    "            return f\"{self.nom}: {len(self.donnees)} lignes\"\n",
    "        return f\"{self.nom}: Aucune donnée\"\n",
    "\n",
    "# Utilisation des méthodes\n",
    "processeur = ProcesseurDonnees(\"Ventes\")\n",
    "# processeur.charger_donnees(\"ventes.csv\")  # Appel de méthode\n",
    "resume = processeur.obtenir_resume()      # Appel de méthode\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Constructeur : L'Initialisation de l'Objet**\n",
    "\n",
    "Le constructeur est une méthode spéciale qui s'exécute automatiquement lors de la création d'une instance. En Python, le constructeur s'appelle `__init__`. Il permet d'initialiser les attributs de l'objet avec des valeurs de départ.\n",
    "\n",
    "**Pourquoi utiliser un constructeur :**\n",
    "- Garantir que chaque instance démarre dans un état cohérent\n",
    "- Forcer la fourniture de paramètres essentiels\n",
    "- Initialiser les attributs avec des valeurs par défaut\n",
    "- Effectuer des validations lors de la création"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class ProcesseurDonnees:\n",
    "    def __init__(self, nom, format_fichier=\"csv\"):\n",
    "        # Validation des paramètres\n",
    "        if not nom:\n",
    "            raise ValueError(\"Le nom ne peut pas être vide\")\n",
    "        \n",
    "        # Initialisation des attributs obligatoires\n",
    "        self.nom = nom\n",
    "        self.format_fichier = format_fichier\n",
    "        \n",
    "        # Initialisation des attributs avec valeurs par défaut\n",
    "        self.donnees = None\n",
    "        self.date_creation = datetime.now()\n",
    "        self.nb_operations = 0\n",
    "        \n",
    "        print(f\"Processeur '{nom}' créé pour format {format_fichier}\")\n",
    "\n",
    "# Le constructeur s'exécute automatiquement\n",
    "processeur = ProcesseurDonnees(\"Ventes\", \"parquet\")\n",
    "# Affiche: Processeur 'Ventes' créé pour format parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types de constructeurs :**\n",
    "\n",
    "| Type | Description | Exemple |\n",
    "|------|-------------|------|\n",
    "| **Simple** | Initialisation basique | `__init__(self, nom)` |\n",
    "| **Avec validation** | Vérifie les paramètres | Contrôle des valeurs |\n",
    "| **Avec valeurs par défaut** | Paramètres optionnels | `format=\"csv\"` |\n",
    "| **Avec calculs** | Initialise des valeurs dérivées | Calcul de métadonnées |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Encapsulation : Regroupement Logique**\n",
    "\n",
    "L'encapsulation consiste à regrouper les données (attributs) et les actions (méthodes) qui les manipulent dans une même classe. Cela crée une unité logique et cohérente.\n",
    "\n",
    "**Avantages de l'encapsulation :**\n",
    "- Organisation claire du code\n",
    "- Réutilisabilité des composants\n",
    "- Facilité de maintenance\n",
    "- Protection des données internes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcesseurDonnees:\n",
    "    def __init__(self, nom):\n",
    "        self.nom = nom\n",
    "        self._donnees = None        # Attribut \"privé\" (convention)\n",
    "        self._nb_operations = 0     # Attribut \"privé\"\n",
    "    \n",
    "    def charger(self, fichier):\n",
    "        \"\"\"Méthode publique pour charger des données\"\"\"\n",
    "        self._donnees = self._lire_fichier(fichier)\n",
    "        self._incrementer_operations()\n",
    "    \n",
    "    def _lire_fichier(self, fichier):\n",
    "        \"\"\"Méthode privée (convention avec _)\"\"\"\n",
    "        import pandas as pd\n",
    "        return pd.read_csv(fichier)\n",
    "    \n",
    "    def _incrementer_operations(self):\n",
    "        \"\"\"Méthode privée pour compter les opérations\"\"\"\n",
    "        self._nb_operations += 1\n",
    "    \n",
    "    @property\n",
    "    def nb_operations(self):\n",
    "        \"\"\"Propriété pour accéder au compteur (lecture seule)\"\"\"\n",
    "        return self._nb_operations\n",
    "\n",
    "# Utilisation\n",
    "processeur = ProcesseurDonnees(\"Ventes\")\n",
    "# processeur.charger(\"ventes.csv\")  # Méthode publique\n",
    "print(processeur.nb_operations)   # Accès via propriété\n",
    "# processeur._nb_operations = 100  # Déconseillé (mais possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Organisation du Code et Fichiers Python**\n",
    "\n",
    "### **Structure de projet recommandée avec Poetry**\n",
    "\n",
    "```\n",
    "mon_projet_data/\n",
    "├── src/                    # Code source principal\n",
    "│   ├── __init__.py        # Package principal\n",
    "│   ├── models.py          # Classes métier\n",
    "│   ├── utils.py           # Fonctions utilitaires\n",
    "│   ├── config.py          # Configuration\n",
    "│   └── exceptions.py      # Exceptions personnalisées\n",
    "├── tests/                 # Tests unitaires\n",
    "│   ├── __init__.py\n",
    "│   ├── test_models.py\n",
    "│   └── conftest.py        # Configuration des tests\n",
    "├── data/                  # Dossiers de données\n",
    "│   ├── raw/              # Données brutes\n",
    "│   ├── processed/        # Données traitées\n",
    "│   └── output/           # Résultats finaux\n",
    "├── scripts/              # Scripts d'automatisation\n",
    "│   └── run_pipeline.py   # Lancement du pipeline\n",
    "├── docs/                 # Documentation\n",
    "├── pyproject.toml        # Configuration Poetry et métadonnées\n",
    "├── poetry.lock           # Versions exactes des dépendances\n",
    "├── .env                  # Variables d'environnement\n",
    "├── .gitignore           # Fichiers à ignorer par Git\n",
    "├── README.md            # Documentation du projet\n",
    "└── main.py              # Point d'entrée principal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fichiers Python classiques et leurs rôles**\n",
    "\n",
    "| Fichier | Rôle | Contenu typique |\n",
    "|---------|------|----------------|\n",
    "| `__init__.py` | Transforme un dossier en package | Imports, version, configuration |\n",
    "| `main.py` | Point d'entrée principal | Fonction main(), orchestration |\n",
    "| `config.py` | Configuration centralisée | Constantes, paramètres, chemins |\n",
    "| `utils.py` | Fonctions utilitaires | Helpers, fonctions communes |\n",
    "| `models.py` | Classes métier | Classes principales du domaine |\n",
    "| `exceptions.py` | Exceptions personnalisées | Erreurs spécifiques au projet |\n",
    "| `constants.py` | Constantes globales | Valeurs fixes, énumérations |\n",
    "| `settings.py` | Paramètres d'application | Configuration avancée |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Gestion d'Erreurs Robuste**\n",
    "\n",
    "### **Pourquoi gérer les erreurs**\n",
    "\n",
    "En Data Engineering, nous travaillons avec des fichiers externes, des connexions réseau, et des données imprévisibles. Une bonne gestion d'erreurs permet à votre pipeline de continuer à fonctionner même quand quelque chose se passe mal.\n",
    "\n",
    "### **Structure try/except/finally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def charger_donnees_robuste(chemin_fichier):\n",
    "    \"\"\"Charge des données avec gestion complète des erreurs\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        # Tentative de chargement normal\n",
    "        donnees = pd.read_csv(chemin_fichier)\n",
    "        if donnees.empty:\n",
    "            raise ValueError(\"Le fichier est vide\")\n",
    "        logger.info(f\"Chargement réussi: {len(donnees)} lignes\")\n",
    "        return donnees\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Fichier non trouvé: {chemin_fichier}\")\n",
    "        return None\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.warning(f\"Fichier CSV vide: {chemin_fichier}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur inattendue: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        logger.info(\"Fin de la tentative de chargement\")\n",
    "\n",
    "# Utilisation\n",
    "# donnees = charger_donnees_robuste(\"ventes.csv\")\n",
    "# if donnees is not None and not donnees.empty:\n",
    "#     print(\"Données prêtes pour le traitement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types d'erreurs courantes en Data Engineering**\n",
    "\n",
    "| Type d'erreur | Cause | Solution |\n",
    "|---------------|-------|----------|\n",
    "| `FileNotFoundError` | Fichier inexistant | Vérifier le chemin, proposer un fichier alternatif |\n",
    "| `UnicodeDecodeError` | Problème d'encodage | Tester différents encodages (utf-8, latin-1) |\n",
    "| `pd.errors.ParserError` | Format CSV invalide | Ajuster les paramètres de lecture |\n",
    "| `MemoryError` | Fichier trop volumineux | Lecture par chunks |\n",
    "| `ValueError` | Données invalides | Validation et nettoyage |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Manipulation de Données avec Pandas et NumPy**\n",
    "\n",
    "### **DataFrames et Opérations de Base**\n",
    "\n",
    "Un DataFrame est la structure de données principale de Pandas. C'est comme un tableau Excel en Python, avec des lignes et des colonnes nommées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Création d'un DataFrame simple\n",
    "donnees = {\n",
    "    'nom': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35],\n",
    "    'salaire': [3000, 3500, 4000]\n",
    "}\n",
    "df = pd.DataFrame(donnees)\n",
    "\n",
    "# Exploration basique\n",
    "print(df.head())        # Premières lignes\n",
    "print(df.info())        # Informations générales\n",
    "print(df.describe())    # Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Méthodes essentielles de Pandas**\n",
    "\n",
    "| Méthode | Usage | Description |\n",
    "|---------|-------|-------------|\n",
    "| `.head(n)` | `df.head(5)` | Affiche les n premières lignes |\n",
    "| `.tail(n)` | `df.tail(5)` | Affiche les n dernières lignes |\n",
    "| `.info()` | `df.info()` | Informations sur le DataFrame |\n",
    "| `.describe()` | `df.describe()` | Statistiques descriptives |\n",
    "| `.shape` | `df.shape` | Dimensions (lignes, colonnes) |\n",
    "| `.columns` | `df.columns` | Noms des colonnes |\n",
    "| `.dtypes` | `df.dtypes` | Types de données |\n",
    "| `.isnull()` | `df.isnull()` | Détecte les valeurs manquantes |\n",
    "| `.dropna()` | `df.dropna()` | Supprime les valeurs manquantes |\n",
    "| `.fillna()` | `df.fillna(0)` | Remplace les valeurs manquantes |\n",
    "| `.groupby()` | `df.groupby('colonne')` | Groupe les données |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NumPy pour les Calculs Numériques**\n",
    "\n",
    "NumPy est la base de l'écosystème scientifique Python. Ses arrays sont beaucoup plus efficaces que les listes Python pour les calculs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Conversion en array NumPy\n",
    "salaires = df['salaire'].values\n",
    "\n",
    "# Statistiques avec NumPy\n",
    "moyenne = np.mean(salaires)\n",
    "mediane = np.median(salaires)\n",
    "ecart_type = np.std(salaires)\n",
    "minimum = np.min(salaires)\n",
    "maximum = np.max(salaires)\n",
    "\n",
    "print(f\"Moyenne: {moyenne}\")\n",
    "print(f\"Médiane: {mediane}\")\n",
    "print(f\"Écart-type: {ecart_type}\")\n",
    "\n",
    "# Détection d'outliers\n",
    "z_scores = np.abs((salaires - moyenne) / ecart_type)\n",
    "outliers = salaires[z_scores > 2]  # Valeurs à plus de 2 écarts-types\n",
    "print(f\"Outliers: {outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avantages de NumPy vs listes Python :**\n",
    "\n",
    "| Aspect | Listes Python | NumPy Arrays |\n",
    "|--------|---------------|-------------|\n",
    "| **Vitesse** | Lent | 10-100x plus rapide |\n",
    "| **Mémoire** | Beaucoup | Peu |\n",
    "| **Syntaxe** | Boucles explicites | Opérations vectorisées |\n",
    "| **Fonctions** | Limitées | Bibliothèque mathématique complète |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Formats de Fichiers**\n",
    "\n",
    "### **Format CSV : Lecture robuste**\n",
    "\n",
    "Le format CSV est omniprésent mais peut poser des défis d'encodage et de format. La clé est de gérer automatiquement ces problèmes.\n",
    "\n",
    "**Problèmes courants avec les CSV :**\n",
    "- Encodage incorrect (UTF-8 vs Latin-1)\n",
    "- Séparateurs différents (virgule, point-virgule, tabulation)\n",
    "- Caractères spéciaux dans les données\n",
    "- Lignes de commentaires\n",
    "- Formats numériques localisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_csv_robuste(fichier):\n",
    "    \"\"\"Lecture robuste avec gestion automatique des problèmes\"\"\"\n",
    "    encodages = ['utf-8', 'latin-1', 'cp1252']\n",
    "    \n",
    "    for encodage in encodages:\n",
    "        try:\n",
    "            df = pd.read_csv(fichier, encoding=encodage)\n",
    "            print(f\"Lecture réussie avec {encodage}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    raise ValueError(\"Impossible de lire le fichier\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# df = lire_csv_robuste(\"mon_fichier.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paramètres avancés de pd.read_csv()**\n",
    "\n",
    "**Paramètres de lecture de base :**\n",
    "\n",
    "| Paramètre | Description | Exemple |\n",
    "|-----------|-------------|------|\n",
    "| `sep` | Séparateur de colonnes | `sep=';'` |\n",
    "| `encoding` | Encodage du fichier | `encoding='latin-1'` |\n",
    "| `header` | Ligne des en-têtes | `header=0` (première ligne) |\n",
    "| `skiprows` | Lignes à ignorer | `skiprows=2` |\n",
    "| `comment` | Caractère de commentaire | `comment='#'` |\n",
    "\n",
    "**Paramètres de sélection :**\n",
    "\n",
    "| Paramètre | Description | Exemple |\n",
    "|-----------|-------------|------|\n",
    "| `usecols` | Colonnes à lire | `usecols=['nom', 'age']` |\n",
    "| `nrows` | Nombre de lignes | `nrows=1000` |\n",
    "| `dtype` | Types de données | `dtype={'age': 'int32'}` |\n",
    "| `parse_dates` | Colonnes de dates | `parse_dates=['date_naissance']` |\n",
    "\n",
    "**Paramètres de performance :**\n",
    "\n",
    "| Paramètre | Description | Exemple |\n",
    "|-----------|-------------|------|\n",
    "| `chunksize` | Lecture par blocs | `chunksize=10000` |\n",
    "| `low_memory` | Optimisation mémoire | `low_memory=False` |\n",
    "| `compression` | Fichiers compressés | `compression='gzip'` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimisation de l'écriture CSV**\n",
    "\n",
    "L'écriture efficace de fichiers CSV est importante pour plusieurs raisons. La performance devient critique quand vous traitez de gros volumes de données. Réduire le temps d'écriture permet d'accélérer vos pipelines. Minimiser l'espace disque utilisé réduit les coûts de stockage et améliore les transferts. Assurer la lisibilité par d'autres systèmes garantit l'interopérabilité. Éviter la corruption des données protège l'intégrité de vos résultats.\n",
    "\n",
    "**Stratégies d'optimisation :**\n",
    "\n",
    "| Stratégie | Avantage | Quand l'utiliser |\n",
    "|-----------|----------|------------------|\n",
    "| **Compression** | Réduit la taille de 50-80% | Stockage long terme |\n",
    "| **Types optimisés** | Moins de mémoire | Gros DataFrames |\n",
    "| **Format numérique** | Fichiers plus petits | Données décimales |\n",
    "| **Écriture par chunks** | Gestion mémoire | Très gros volumes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écriture optimisée\n",
    "df.to_csv(\n",
    "    \"sortie_optimisee.csv\",\n",
    "    index=False,\n",
    "    float_format='%.2f',      # Limite les décimales\n",
    "    compression='gzip',       # Compression automatique\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Formats JSON et Parquet**\n",
    "\n",
    "**JSON : Flexibilité et lisibilité**\n",
    "\n",
    "Le JSON est idéal pour les données semi-structurées, les APIs et échanges web, la configuration et métadonnées, et les données imbriquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture JSON flexible\n",
    "# data = pd.read_json(\"donnees.json\")\n",
    "\n",
    "# Écriture JSON propre\n",
    "df.to_json(\"sortie.json\", orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet : Performance et compression**\n",
    "\n",
    "Parquet est le format de référence pour le Big Data. Sa compression permet des fichiers 5-10x plus petits que CSV. Sa vitesse de lecture est ultra-rapide. Il préserve les types de données. Il permet la lecture sélective par colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écriture Parquet\n",
    "df.to_parquet(\"donnees.parquet\", compression='snappy')\n",
    "\n",
    "# Lecture avec sélection de colonnes\n",
    "df_partiel = pd.read_parquet(\"donnees.parquet\", columns=['nom', 'age'])\n",
    "print(df_partiel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparaison des formats :**\n",
    "\n",
    "| Format | Taille | Vitesse lecture | Lisibilité | Usage recommandé |\n",
    "|--------|--------|-----------------|------------|------------------|\n",
    "| **CSV** | Grande | Moyenne | Excellente | Échange, analyse manuelle |\n",
    "| **JSON** | Moyenne | Moyenne | Bonne | APIs, données imbriquées |\n",
    "| **Parquet** | Petite | Très rapide | Nulle | Big Data, performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Pipeline ETL et Bonnes Pratiques**\n",
    "\n",
    "### **Architecture ETL**\n",
    "\n",
    "Un pipeline ETL (Extract, Transform, Load) suit toujours la même structure. L'extraction récupère les données depuis les sources. La transformation nettoie et transforme les données. Le chargement sauvegarde les données traitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class PipelineETL:\n",
    "    def __init__(self, source, destination):\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.donnees = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def extraire(self):\n",
    "        \"\"\"Étape 1: Extraction des données\"\"\"\n",
    "        self.donnees = pd.read_csv(self.source)\n",
    "        self.logger.info(f\"Extraction: {len(self.donnees)} lignes\")\n",
    "    \n",
    "    def transformer(self):\n",
    "        \"\"\"Étape 2: Transformation des données\"\"\"\n",
    "        # Nettoyage\n",
    "        self.donnees = self.donnees.dropna()\n",
    "        \n",
    "        # Nouvelles colonnes\n",
    "        if 'quantite' in self.donnees.columns and 'prix' in self.donnees.columns:\n",
    "            self.donnees['total'] = self.donnees['quantite'] * self.donnees['prix']\n",
    "        \n",
    "        self.logger.info(f\"Transformation: {len(self.donnees)} lignes finales\")\n",
    "    \n",
    "    def charger(self):\n",
    "        \"\"\"Étape 3: Chargement des données\"\"\"\n",
    "        self.donnees.to_csv(self.destination, index=False)\n",
    "        self.logger.info(f\"Chargement: données sauvées dans {self.destination}\")\n",
    "    \n",
    "    def executer(self):\n",
    "        \"\"\"Exécution complète du pipeline\"\"\"\n",
    "        try:\n",
    "            self.extraire()\n",
    "            self.transformer()\n",
    "            self.charger()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur dans le pipeline: {e}\")\n",
    "            return False\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# pipeline = PipelineETL(\"donnees_source.csv\", \"donnees_traitees.csv\")\n",
    "# succes = pipeline.executer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bonnes Pratiques pour les Pipelines**\n",
    "\n",
    "#### **Gestion d'Erreurs et Résilience**\n",
    "\n",
    "Un pipeline robuste doit pouvoir gérer les pannes et reprendre là où il s'est arrêté. Cette résilience est cruciale en production où les interruptions sont fréquentes.\n",
    "\n",
    "La gestion granulaire des erreurs permet d'identifier précisément où un problème survient. Encapsuler chaque méthode dans des blocs try/catch permet de gérer les erreurs spécifiques à chaque étape. Le retry automatique gère les échecs temporaires comme les problèmes réseau. Un décorateur avec backoff exponentiel évite de surcharger les systèmes défaillants. Les checkpoints sauvegardent l'état à chaque étape majeure. Des fichiers de progression permettent de reprendre un pipeline interrompu. Le rollback annule les modifications en cas d'échec critique. La sauvegarde des états permet de revenir à un point stable.\n",
    "\n",
    "| Stratégie | Description | Implémentation |\n",
    "|-----------|-------------|----------------|\n",
    "| **Try/Catch granulaire** | Gérer les erreurs à chaque étape | Encapsuler chaque méthode |\n",
    "| **Retry automatique** | Réessayer en cas d'échec temporaire | Décorateur avec backoff |\n",
    "| **Checkpoints** | Sauvegarder l'état à chaque étape | Fichiers de progression |\n",
    "| **Rollback** | Annuler en cas d'échec critique | Sauvegarde des états |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validation et Contrôle de Qualité**\n",
    "\n",
    "Valider les données à chaque étape permet de détecter les problèmes tôt. Cette approche préventive évite que des données corrompues se propagent dans tout le pipeline.\n",
    "\n",
    "La validation structurelle vérifie le format et le schéma des données. Elle contrôle la présence des colonnes requises et la cohérence des types de données. La validation métier assure la cohérence business. Elle vérifie que les âges sont positifs, les dates valides, les montants cohérents. La validation statistique détecte les anomalies dans les données. Elle identifie les outliers et analyse les distributions. La validation de complétude contrôle les données manquantes. Elle mesure les taux de remplissage et identifie les valeurs nulles problématiques.\n",
    "\n",
    "| Type | Objectif | Exemples de contrôles |\n",
    "|------|----------|----------------------|\n",
    "| **Structurelle** | Format et schéma | Colonnes requises, types de données |\n",
    "| **Métier** | Cohérence business | Âges positifs, dates valides |\n",
    "| **Statistique** | Anomalies dans les données | Outliers, distributions |\n",
    "| **Complétude** | Données manquantes | Taux de remplissage, valeurs nulles |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Performance et Optimisation**\n",
    "\n",
    "Optimiser pour traiter de gros volumes efficacement devient essentiel quand vos données grandissent. Les techniques d'optimisation permettent de maintenir des temps de traitement acceptables.\n",
    "\n",
    "La lecture par chunks maintient une utilisation mémoire constante. Cette technique est indispensable pour les fichiers plus volumineux que la RAM disponible. Les types optimisés réduisent l'utilisation mémoire. Cette optimisation est particulièrement efficace sur les DataFrames volumineux. La parallélisation accroît la vitesse de traitement. Elle est utile pour les opérations indépendantes qui peuvent s'exécuter simultanément. Les formats efficaces accélèrent les opérations d'entrée/sortie. Ils sont recommandés pour le stockage intermédiaire.\n",
    "\n",
    "| Technique | Avantage | Quand l'utiliser |\n",
    "|-----------|----------|------------------|\n",
    "| **Lecture par chunks** | Mémoire constante | Fichiers > RAM disponible |\n",
    "| **Types optimisés** | Moins de mémoire | DataFrames volumineux |\n",
    "| **Parallélisation** | Vitesse accrue | Opérations indépendantes |\n",
    "| **Formats efficaces** | I/O plus rapide | Stockage intermédiaire |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Monitoring et Observabilité**\n",
    "\n",
    "Surveiller l'exécution permet de détecter et diagnostiquer les problèmes rapidement. Cette observabilité est essentielle pour maintenir des pipelines en production.\n",
    "\n",
    "Le temps d'exécution indique les problèmes de performance. Un dépassement de 50% du temps normal signale souvent un problème. L'utilisation mémoire révèle les fuites et les goulots d'étranglement. Un usage supérieur à 80% de la RAM disponible nécessite une attention. Le taux d'erreur mesure la qualité du traitement. Plus de 1% des enregistrements en erreur indique un problème systémique. Le volume de données détecte les anomalies dans les sources. Une variation de plus ou moins 20% du volume attendu mérite investigation.\n",
    "\n",
    "| Métrique | Importance | Seuils typiques |\n",
    "|----------|------------|----------------|\n",
    "| **Temps d'exécution** | Performance | +50% du temps normal |\n",
    "| **Utilisation mémoire** | Stabilité | >80% de la RAM |\n",
    "| **Taux d'erreur** | Qualité | >1% des enregistrements |\n",
    "| **Volume de données** | Cohérence | ±20% du volume attendu |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuration et Environnements**\n",
    "\n",
    "Séparer la configuration du code facilite le déploiement dans différents environnements. Cette séparation permet d'adapter le comportement sans modifier le code source.\n",
    "\n",
    "Les constantes stockent les valeurs fixes comme les formats de date et les seuils métier. L'environnement configure les paramètres de déploiement comme les chemins, URLs et credentials. Le runtime gère les paramètres d'exécution fournis par l'utilisateur.\n",
    "\n",
    "| Niveau | Portée | Exemples |\n",
    "|--------|--------|----------|\n",
    "| **Constantes** | Valeurs fixes | Formats de date, seuils |\n",
    "| **Environnement** | Déploiement | Chemins, URLs, credentials |\n",
    "| **Runtime** | Exécution | Paramètres utilisateur |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tests et Qualité**\n",
    "\n",
    "Tester chaque composant assure la fiabilité du pipeline. Cette approche systématique prévient les régressions et facilite la maintenance.\n",
    "\n",
    "Les tests unitaires vérifient les fonctions individuelles. Ils testent chaque méthode isolément avec des données contrôlées. Les tests d'intégration valident les composants ensemble. Ils vérifient que le pipeline complet fonctionne correctement. Les tests de données contrôlent la qualité des résultats. Ils valident que les outputs respectent les contraintes métier.\n",
    "\n",
    "| Type | Objectif | Exemples |\n",
    "|------|----------|----------|\n",
    "| **Unitaires** | Fonctions individuelles | Test d'une méthode |\n",
    "| **Intégration** | Composants ensemble | Pipeline complet |\n",
    "| **Données** | Qualité des résultats | Validation des outputs |\n",
    "\n",
    "Ces bonnes pratiques garantissent que vos pipelines de données sont robustes, maintenables et prêts pour la production. L'investissement initial dans ces pratiques se rentabilise rapidement par la réduction des bugs et la facilité de maintenance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
